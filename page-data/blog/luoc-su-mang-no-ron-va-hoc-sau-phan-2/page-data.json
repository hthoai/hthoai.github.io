{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/luoc-su-mang-no-ron-va-hoc-sau-phan-2/","result":{"data":{"markdownRemark":{"html":"<h1>Phần 2: Thời kỳ nở hoa của mạng nơ-ron (những năm 1980-2000)</h1>\n<h2>Khả năng thị giác</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 320px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b90bda6c922792609e3e2cf015dab350/cb69c/ba4de6fd8abd7c43090b4b24efe35159e1c3603c.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.35294117647059%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAv/EABUBAQEAAAAAAAAAAAAAAAAAAAMC/9oADAMBAAIQAxAAAAHVU1FigWIX/8QAGhABAQACAwAAAAAAAAAAAAAAAQIAAxExMv/aAAgBAQABBQJ7auRtrKOHZ5z/xAAXEQEAAwAAAAAAAAAAAAAAAAAAARFR/9oACAEDAQE/AdXL/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BR//EABkQAAEFAAAAAAAAAAAAAAAAABAAESEyQv/aAAgBAQAGPwKzrIg//8QAGxAAAgIDAQAAAAAAAAAAAAAAAREAIRAxUUH/2gAIAQEAAT8hEi5V5qMxgwyoACILU9OrH//aAAwDAQACAAMAAAAQC8//xAAXEQEBAQEAAAAAAAAAAAAAAAABABFB/9oACAEDAQE/ENRQy3b/xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQIBAT8QwcGAX//EAB0QAQEBAQABBQAAAAAAAAAAAAERIQAxEFFhcdH/2gAIAQEAAT8QFU48wfA6ZUEoFS+OKnQTL+8FUDhy7vRc661z69vT/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ba4de6fd8abd7c43090b4b24efe35159e1c3603c.jpeg\"\n        title=\"ba4de6fd8abd7c43090b4b24efe35159e1c3603c.jpeg\"\n        src=\"/static/b90bda6c922792609e3e2cf015dab350/cb69c/ba4de6fd8abd7c43090b4b24efe35159e1c3603c.jpg\"\n        srcset=\"/static/b90bda6c922792609e3e2cf015dab350/651be/ba4de6fd8abd7c43090b4b24efe35159e1c3603c.jpg 170w,\n/static/b90bda6c922792609e3e2cf015dab350/cb69c/ba4de6fd8abd7c43090b4b24efe35159e1c3603c.jpg 320w\"\n        sizes=\"(max-width: 320px) 100vw, 320px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Mạng LeNet của Yann LeCun (</em><em><a href=\"http://yann.lecun.com/exdb/lenet/gifs/asamples.gif\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Nhờ những hé mở về cách huấn luyện mạng nơ-ron đa lớp, chủ đề này một lần nữa nóng trở lại và tham vọng cao cả của Rosenblatt dường như đã trong tầm tay. Chỉ đến năm 1989, một phát hiện quan trọng khác (ngày nay được trích dẫn phổ biến trong các tài liệu của ngành) được xuất bản: “Mạng truyền xuôi đa lớp là các máy xấp xỉ phổ quát”. Về cơ bản, nó đã được chứng minh toán học rằng nhiều lớp cho phép mạng nơ-ron thực thi bất kỳ hàm nào, và hiển nhiên là cả XOR.</p>\n<p>Trên lý thuyết là thế, khi bộ nhớ vô tận và sức mạnh tính toán cần thiết để thực hiện nằm trong trí tưởng tượng, thực tế thì lan truyền ngược có cho phép mạng nơ-ron được dùng cho bất kỳ thứ gì? Cũng trong năm 1989, Yann LeCun và các cộng sự ở Bell Labs (phòng thí nghiệm thuộc AT&#x26;T) đã chứng minh một ứng dụng thực tế rất quan trọng của lan truyền ngược trong ”Backpropagation Applied to Handwritten Zip Code Recognition” (Ứng dụng lan truyền ngược cho bài toán nhận dạng mã bưu chính viết tay). Bạn có thể nghĩ rằng việc máy tính hiểu được chính xác các chữ số viết tay chẳng lấy làm ấn tượng gì, nhưng trước khi bài báo này được xuất bản, những nét vẽ nguệch ngoạc của con người thật sự làm khó bộ óc ngăn nắp của những chiếc máy tính. Bài báo khi thực nghiệm trên bộ dữ liệu lớn của Dịch vụ Bưu chính Hoa Kỳ (USPS) đã cho thấy mạng nơ-ron hoàn toàn có khả năng thực hiện tác vụ này. Quan trọng hơn cả, đây là vấn đề đầu tiên nêu bật nhu cầu thực tế về những sửa đổi quan trọng của mạng nơ-ron ngoài kỹ thuật lan truyền ngược ban đầu, để dần tiến đến những mô hình học sâu hiện đại.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 411px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/216b393586dc1de48e09ff1e80daa69b/d3fc3/771f093efe1ade75e4a26f0851e89f14fc6392f0.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 133.52941176470588%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAbABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHvSMYwlTJuAf/EABwQAQABBAMAAAAAAAAAAAAAAAEAAxARMhIhQf/aAAgBAQABBQL2EychGz1BLVNKOs//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAaEAACAgMAAAAAAAAAAAAAAAAQEQABIEGB/9oACAEBAAY/Ait4MXOj/8QAHRABAAICAgMAAAAAAAAAAAAAAQARITFBYRBRcf/aAAgBAQABPyHdIuXc0mj4XFH2YZic1L8lLtBsxGjDTFb6p4//2gAMAwEAAgADAAAAEAMODP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EAB0QAQADAAMAAwAAAAAAAAAAAAEAESExQXEQUYH/2gAIAQEAAT8QeUdfcyLFPTXUQnb9Zc7K2ZleyiWl0zYKMT8iAFg1QtYnajhkAHQx3QGI0kYS4g8z4//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"771f093efe1ade75e4a26f0851e89f14fc6392f0.jpeg\"\n        title=\"771f093efe1ade75e4a26f0851e89f14fc6392f0.jpeg\"\n        src=\"/static/216b393586dc1de48e09ff1e80daa69b/d3fc3/771f093efe1ade75e4a26f0851e89f14fc6392f0.jpg\"\n        srcset=\"/static/216b393586dc1de48e09ff1e80daa69b/651be/771f093efe1ade75e4a26f0851e89f14fc6392f0.jpg 170w,\n/static/216b393586dc1de48e09ff1e80daa69b/d30a3/771f093efe1ade75e4a26f0851e89f14fc6392f0.jpg 340w,\n/static/216b393586dc1de48e09ff1e80daa69b/d3fc3/771f093efe1ade75e4a26f0851e89f14fc6392f0.jpg 411w\"\n        sizes=\"(max-width: 411px) 100vw, 411px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Hình ảnh trực quan về cách mạng nơ-ron này hoạt động (</em><em><a href=\"http://image.slidesharecdn.com/bp2slides-090922011749-phpapp02/95/the-back-propagation-learning-algorithm-10-728.jpg?cb=1253582278\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Cụ thể hơn, lớp ẩn đầu tiên của mạng nơ-ron là lớp tích chập, thay vì mỗi nơ-ron có một trọng số khác nhau với mỗi pixel ảnh đầu vào (40x60=2400 trọng số), thì các nơ-ron chỉ có một tập trọng số nhỏ (5x5=25) được áp dụng trên các vùng nhỏ có cùng kích thước trên ảnh. Vì vậy, nếu có 4 nơ-ron khác nhau cho việc học để phát hiện các đường 45 độ ở mỗi góc trong số 4 góc của ảnh đầu vào, thì một nơ-ron đơn lẻ có thể làm được điều này trên các vùng cục bộ của ảnh. Bước đầu tiên ở các lớp đều tương tự nhau, tuy nhiên đầu vào sẽ là các đặc trưng cục bộ đã tìm được ở lớp ẩn trước đó, thay vì những pixel, và do đó có thể “thấy“ những phần lớn hơn liên tiếp của ảnh vì chúng đang kết hợp thông tin về các vùng cục bộ liên tiếp này. Sau cùng, hai lớp cuối chỉ là các lớp mạng nơ-ron bình thường (mạng truyền xuôi) sử dụng các đặc trưng bậc cao hơn đã được tạo ra bởi các lớp tích chập để xác định ảnh đầu vào tương ứng với chữ số nào. Phương pháp được đề xuất trong bài báo năm 1989 này đã trở thành cơ sở của các hệ thống đọc-kiểm được triển khai trên toàn nước Mỹ, như đã được LeCun chứng minh trong video này:</p>\n<p><div class=\"gatsby-resp-iframe-wrapper\" style=\"padding-bottom: 56.42857142857143%; position: relative; height: 0; overflow: hidden; \" > <div class=\"embedVideo-container\"> <iframe title src=\"https://www.youtube.com/embed/FwFduRA_L6Q?rel=0\" class=\"embedVideo-iframe\" style=\"border:0; position: absolute; top: 0; left: 0; width: 100%; height: 100%; \" allowfullscreen></iframe> </div> </div></p>\n<p>Nếu không xét rõ về mặt toán học, lý do tại sao các lớp tích chập như trên lại hữu ích là khá trực quan: nếu không có những ràng buộc như vậy, mạng sẽ phải học những thứ đơn giản giống nhau (như phát hiện đường 45 độ, vòng tròn nhỏ) rất nhiều lần trên mỗi phần của ảnh. Nhưng với những ràng buộc này, chỉ cần một nơ-ron cho việc học một đặc trưng đơn giản, số lượng trọng số vì thế cũng ít đi nhiều, và việc tính toán sẽ diễn ra nhanh hơn rất nhiều. Hơn nữa, vì vị trí chính xác đến từng pixel của các đặc trưng như vậy không quan trọng nên nơ-ron về cơ bản có thể bỏ qua các tập con lân cận của ảnh - lấy mẫu con (subsampling), ngày nay được xem là một kiểu gộp - khi áp dụng trọng số, sẽ còn giảm thời gian huấn luyện hơn nữa. Việc bổ sung hai loại lớp này - tích chập (convolutional) và gộp (pooling) là sự khác biệt cơ bản của mạng nơ-ron tích chập (CNNs/ConvNets) so với mạng nơ-ron đơn thuần.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 560px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/39db2308200951139144e4d8d1944da2/9342c/a64a1997569591b78c7befd35620aea526993ead.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.294117647058826%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBBAX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHZXYWOID//xAAZEAEAAgMAAAAAAAAAAAAAAAABAjEQEUH/2gAIAQEAAQUCky2MsNcK/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxAAAwEAAAAAAAAAAAAAAAAAARAhAP/aAAgBAQAGPwKCasr/xAAaEAADAAMBAAAAAAAAAAAAAAAAATEQESFh/9oACAEBAAE/IVW7p7ROdMULYpn/2gAMAwEAAgADAAAAECQP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHhABAAIBBAMAAAAAAAAAAAAAAQAhERAxUaFBcfD/2gAIAQEAAT8QDhAI7rWY9RVwO0bmOfuYlC0+Z1Iduk//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"a64a1997569591b78c7befd35620aea526993ead.jpeg\"\n        title=\"a64a1997569591b78c7befd35620aea526993ead.jpeg\"\n        src=\"/static/39db2308200951139144e4d8d1944da2/9342c/a64a1997569591b78c7befd35620aea526993ead.jpg\"\n        srcset=\"/static/39db2308200951139144e4d8d1944da2/651be/a64a1997569591b78c7befd35620aea526993ead.jpg 170w,\n/static/39db2308200951139144e4d8d1944da2/d30a3/a64a1997569591b78c7befd35620aea526993ead.jpg 340w,\n/static/39db2308200951139144e4d8d1944da2/9342c/a64a1997569591b78c7befd35620aea526993ead.jpg 560w\"\n        sizes=\"(max-width: 560px) 100vw, 560px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một ví dụ trực quan về triển khai của CNN (</em><em><a href=\"https://sites.google.com/site/5kk73gpu2013/assignment/cnn\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Vào thời điểm đó, ý tưởng về tích chập được gọi là “chia sẻ trọng số“, và nó đã thực sự được thảo luận trong phân tích mở rộng năm 1986 về kỹ thuật lan truyền ngược của Rumelhart, Hinton, và Williams. Trên thực tế, nó đã xuất hiện từ trước đó nữa, phân tích năm 1969 của Minsky và Papert về Perceptrons đủ kỹ lưỡng để đặt ra vấn đề về việc thúc đẩy ý tưởng này. Nhưng như trước đây, những người khác cũng đã đưa ra những khám phá mới một cách độc lập, cụ thể là Kunihiko Fukushima vào năm 1980 với khái niệm về Neurocognitron. Và cũng như trước đây, những ý tưởng này được lấy cảm hứng từ các nghiên cứu về não bộ:</p>\n<p>LeCun tiếp tục là người đề xuất chính về CNNs tại Bell Labs. Công trình của ông đã giúp thương mại hóa việc sử dụng hệ thống đọc-kiểm vào giữa những năm 90 - các cuộc nói chuyện và phỏng vấn của ông thường bao gồm thực tế rằng “Vào một thời điểm nào đó ở cuối những năm 90, một trong những hệ thống này sẽ đọc từ 10 đến 20% các mã kiểm ở Mỹ.”</p>\n<h2>Tiến đến học không giám sát</h2>\n<p>Tự động hóa những việc lặp lại và hoàn toàn tẻ nhạt như đọc-kiểm là một ví dụ tuyệt vời về những gì học máy có thể được dùng cho. Còn ứng dụng có vẻ ít được trông đợi hơn? Đó là nén (compression). Nghĩa là tìm một biểu diễn nhỏ hơn của một tập hợp dữ liệu mà từ đó dữ liệu gốc có thể được tái cấu trúc lại. Việc học để nén có thể tốt hơn rất nhiều so với các lược đồ nén sẵn có, khi thuật toán học có thể tìm thấy các đặc trưng bên trong dữ liệu mà các phương pháp lược đồ có thể đã bỏ lỡ. Và nó rất dễ để thực hiện, chỉ cần huấn luyện một mạng nơ-ron với một lớp ẩn nhỏ để xuất ra chính đầu vào:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 337px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a7a81249e529ac4db3a2d7384f964f8a/7f446/e59f0dc2a4260aaafb7155ed755c0ce59c40c840.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 90.58823529411765%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAASABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAezVYWCaAD//xAAYEAACAwAAAAAAAAAAAAAAAAAAAiAhMf/aAAgBAQABBQJ6Fyf/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAYEAACAwAAAAAAAAAAAAAAAAAAASEwUf/aAAgBAQAGPwKGbR//xAAbEAACAgMBAAAAAAAAAAAAAAABEQAQITFRYf/aAAgBAQABPyEtsnNwmzgVjykOCACv/9oADAMBAAIAAwAAABCQyDz/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAfEAEAAgICAgMAAAAAAAAAAAABABEhMVGBEHGRodH/2gAIAQEAAT8QclFOw/QzGJJJk1XW4ANV3ELvG+Jhf418wAMR7ziAKAr14//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"e59f0dc2a4260aaafb7155ed755c0ce59c40c840.jpeg\"\n        title=\"e59f0dc2a4260aaafb7155ed755c0ce59c40c840.jpeg\"\n        src=\"/static/a7a81249e529ac4db3a2d7384f964f8a/7f446/e59f0dc2a4260aaafb7155ed755c0ce59c40c840.jpg\"\n        srcset=\"/static/a7a81249e529ac4db3a2d7384f964f8a/651be/e59f0dc2a4260aaafb7155ed755c0ce59c40c840.jpg 170w,\n/static/a7a81249e529ac4db3a2d7384f964f8a/7f446/e59f0dc2a4260aaafb7155ed755c0ce59c40c840.jpg 337w\"\n        sizes=\"(max-width: 337px) 100vw, 337px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một mạng autoencoder - bộ mã hóa tự động (</em><em><a href=\"http://research.chtsai.org/papers/iml-bkp.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Đây là một mạng autoencoder, và là một phương pháp để học nén - mã hóa (encoding) dữ liệu sang định dạng nhỏ gọn hơn, sau đó chuyển nó về đúng dạng ban đầu nhất có thể (auto). Lớp đầu ra sẽ tính kết quả của mạng chỉ dựa trên đầu ra của các nơ-ron ở lớp ẩn, lý tưởng nhất là giống với dữ liệu đầu vào. Vì lớp ẩn có số nơ-ron ít hơn so với lớp đầu vào nên kết quả ở lớp ẩn là một biểu diễn nén của dữ liệu đầu vào mà từ đó có thể tái tạo lại ở lớp đầu ra.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 369px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2a5124ee292f3f5a724af6bed2f6347f/23013/59341d3053df5bf3d414d63279b05eaa2242dc9d.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.11764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAgX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB7c70JUaF/8QAGRAAAwADAAAAAAAAAAAAAAAAAAIRASEy/9oACAEBAAEFApRcVhuZs//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABsQAAEEAwAAAAAAAAAAAAAAAAABAkGREDHh/9oACAEBAAY/Al3ZNkDjuP/EABwQAQACAgMBAAAAAAAAAAAAAAEAEUFRITFxkf/aAAgBAQABPyFNSC8xYiGub9QXcHM/ZTJ723KNT//aAAwDAQACAAMAAAAQ7A//xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAwEBPxBn/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQIBAT8QJ//EACAQAQACAQMFAQAAAAAAAAAAAAEAESExUXFBYYGRocH/2gAIAQEAAT8QEHjT+GcQT5aE6atv114ijPiZgqbHR442iRoa+T3gWg9T/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"59341d3053df5bf3d414d63279b05eaa2242dc9d.jpeg\"\n        title=\"59341d3053df5bf3d414d63279b05eaa2242dc9d.jpeg\"\n        src=\"/static/2a5124ee292f3f5a724af6bed2f6347f/23013/59341d3053df5bf3d414d63279b05eaa2242dc9d.jpg\"\n        srcset=\"/static/2a5124ee292f3f5a724af6bed2f6347f/651be/59341d3053df5bf3d414d63279b05eaa2242dc9d.jpg 170w,\n/static/2a5124ee292f3f5a724af6bed2f6347f/d30a3/59341d3053df5bf3d414d63279b05eaa2242dc9d.jpg 340w,\n/static/2a5124ee292f3f5a724af6bed2f6347f/23013/59341d3053df5bf3d414d63279b05eaa2242dc9d.jpg 369w\"\n        sizes=\"(max-width: 369px) 100vw, 369px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một cái nhìn rõ ràng hơn về việc nén sử dụng autoencoder (</em><em><a href=\"http://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Có một lưu ý nhỏ ở đây: thứ duy nhất chúng ta cần để huấn luyện chỉ là dữ liệu đầu vào. Điều này trái ngược với học có giám sát, cần một tập đầu vào - đầu ra (tức dữ liệu có nhãn) để xấp xỉ gần đúng một hàm có thể tính được đầu ra từ những đầu vào đó. Và thật sự, những autoencoder kiểu này không phải là một dạng của học có giám sát, chúng là một hình thức học không giám sát, chỉ cần một tập dữ liệu đầu vào (không có nhãn) để tìm ra một số cấu trúc ẩn trong dữ liệu đó. Nói cách khác, học không giám sát không xấp xỉ một hàm nào cả, mà nó chuyển đổi dữ liệu đầu vào thành một dạng biểu diễn hữu ích khác của chính dữ liệu đó. Ở đây, biểu diễn này chỉ là một biểu diễn nhỏ hơn của dữ liệu gốc mà từ đó vẫn còn có thể được tái tạo lại, nhưng nó cũng có thể được dùng để tìm các nhóm dữ liệu tương tự nhau (phân cụm) hoặc suy diễn các biến tiềm ẩn (một số khía cạnh của dữ liệu ta biết là có tồn tại nhưng giá trị của chúng thì không biết được).</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 640px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/134d473efba0664620366532a210ba7d/c08c5/ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.588235294117645%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAABwAQH/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAFxAAAwEAAAAAAAAAAAAAAAAAABAhEf/aAAgBAQABPyGYRf/aAAwDAQACAAMAAAAQA8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAaEAACAgMAAAAAAAAAAAAAAAAAAREhMUGB/9oACAEBAAE/EHQ1kdFHRxo//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpeg\"\n        title=\"ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpeg\"\n        src=\"/static/134d473efba0664620366532a210ba7d/c08c5/ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpg\"\n        srcset=\"/static/134d473efba0664620366532a210ba7d/651be/ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpg 170w,\n/static/134d473efba0664620366532a210ba7d/d30a3/ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpg 340w,\n/static/134d473efba0664620366532a210ba7d/c08c5/ff57f3ebd4b90377c2e04147de3a08a0b956dde0.jpg 640w\"\n        sizes=\"(max-width: 640px) 100vw, 640px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Phân cụm, một ứng dụng rất phổ biến của học không giám sát (</em><em><a href=\"https://en.wikipedia.org/wiki/K-means_clustering\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Có những ứng dụng không giám sát khác của mạng nơ-ron được khám phá trước và sau phát hiện về kỹ thuật lan truyền ngược, đáng chú ý nhất là Mạng tự tổ chức (Self Organizing Maps) - tạo ra biểu diễn với số chiều thấp hơn của dữ liệu để việc trực quan hóa dễ dàng hơn, và Thuyết cộng hưởng thích nghi (Adapative Resonance Theory) - có thể học để phân loại bất kỳ dữ liệu nào mà không cần biết trước nhãn. Một cách trực quan, nó có thể học được khá nhiều thứ từ dữ liệu không được gán nhãn. Giả sử bạn có một tập dữ liệu về ảnh chữ số viết tay, không kèm theo nhãn về chữ số tương ứng với mỗi ảnh. Một ảnh với chữ số nào đó trong tập dữ liệu rất có thể trông giống như những ảnh khác cùng nhãn, và dù máy tính không biết những ảnh này tương ứng với chữ số nào, nó vẫn có khả năng chỉ ra rằng chúng là ảnh của cùng một chữ số nhất định. Nhận dạng mẫu như này thật sự là tất cả những gì học máy hướng tới, và được cho là cơ sở cho sức mạnh tuyệt vời của bộ não con người. Nhưng trước hết, hãy quay lại với chủ đề về autoencoder.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/58f9baeace065a71fbb5a46e2e487951/ee745/62ee99734283d996dda7409d84503b2dd3700eee.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQBBv/EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAHn7oUaB//EABkQAAMAAwAAAAAAAAAAAAAAAAECAwAQEf/aAAgBAQABBQLIpOikcO//xAAWEQADAAAAAAAAAAAAAAAAAAABECH/2gAIAQMBAT8BhX//xAAXEQADAQAAAAAAAAAAAAAAAAABEBEh/9oACAECAQE/AaZi/8QAGBABAAMBAAAAAAAAAAAAAAAAAQACESD/2gAIAQEABj8CiWcZnH//xAAZEAEBAAMBAAAAAAAAAAAAAAABABAxQRH/2gAIAQEAAT8h8jFnpmZcuYVdt//aAAwDAQACAAMAAAAQ9z//xAAYEQADAQEAAAAAAAAAAAAAAAAAAREhMf/aAAgBAwEBPxBroOXD/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERIUH/2gAIAQIBAT8Q0JpRXp//xAAcEAEBAQACAwEAAAAAAAAAAAABEQAhUTFBYXH/2gAIAQEAAT8QqDHczxrcp+e95SFL391g4wo0ZlKi9u//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"62ee99734283d996dda7409d84503b2dd3700eee.jpeg\"\n        title=\"62ee99734283d996dda7409d84503b2dd3700eee.jpeg\"\n        src=\"/static/58f9baeace065a71fbb5a46e2e487951/ee745/62ee99734283d996dda7409d84503b2dd3700eee.jpg\"\n        srcset=\"/static/58f9baeace065a71fbb5a46e2e487951/651be/62ee99734283d996dda7409d84503b2dd3700eee.jpg 170w,\n/static/58f9baeace065a71fbb5a46e2e487951/d30a3/62ee99734283d996dda7409d84503b2dd3700eee.jpg 340w,\n/static/58f9baeace065a71fbb5a46e2e487951/ee745/62ee99734283d996dda7409d84503b2dd3700eee.jpg 660w\"\n        sizes=\"(max-width: 660px) 100vw, 660px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Mạng tự tổ chức - ánh xạ một lượng lớn vector đầu vào một lưới các nơ-ron đầu ra, trong đó mỗi nơ-ron là một cụm. Các nơ-ron nằm gần nhau có sự tương đương lớn. (</em><em><a href=\"http://lcdm.astro.illinois.edu/static/code/mlz/MLZ-1.0/doc/html/somz.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Như với chia sẻ trọng số, ý tưởng về autoencoder lần đầu tiên được thảo luận trong phân tích mở rộng năm 1986 về lan truyền ngược, và cũng như chia sẻ trọng số, nó lại xuất hiện trong nhiều nghiên cứu ở những năm tiếp đó, bao gồm cả của chính Hinton. Bài báo với tựa đề khá vui nhộn “Autoencoders, Minimum Description Length, and Helmholts Free Energy” (Autoencoder, độ dài mô tả tối thiểu, và năng lượng tự do Helmholtz) cho rằng “cách tiếp cận tự nhiên với học không giám sát là dùng một mô hình xác định phân phối xác suất trên các vector có thể quan sát được“ và sử dụng mạng nơ-ron để học một mô hình như thế. Vì vậy, đây là một điều thú vị khác mà bạn có thể làm với mạng nơ-ron: xấp xỉ những phân phối xác suất.</p>\n<h2>Học những mô hình xác suất</h2>\n<p>Trên thực tế, trước khi là đồng tác giả của bài báo năm 1986 về thuật toán học lan truyền ngược, Hinton đã nghiên cứu về hướng tiếp cận mạng nơ-ron để học các phân phối xác suất trong “A Learning Algorithm for Boltzmann Machines” (Một thuật toán học cho các máy Boltzmann), 1985. Máy Boltzmann là một loại mạng nơ-ron có các node rất giống với Perceptrons, nhưng thay vì tính đầu ra dựa trên đầu vào và trọng số, mỗi node trong mạng có thể tính xác suất nó có giá trị là 0 hay 1 dựa trên giá trị của các node được kết nối và trọng số tương ứng. Do đó, các node là ngẫu nhiên - chúng hoạt động theo phân phối xác suất, thay vì một cách tất định biết trước. Boltzmann trong bài báo này đề cập đến phân bố xác suất liên quan đến các trạng thái vật chất của hạt trong một hệ thống dựa trên năng lượng hạt và nhiệt độ nhiệt động của hệ thống đó. Phân phối này không chỉ là định nghĩa toán học của của máy Boltzmann mà góp phần làm sáng tỏ nó - các node trong mạng có trạng thái và năng lượng riêng, quá trình học ở đây dựa trên việc tối thiểu hóa năng lượng của hệ thống - một cảm hứng trực tiếp từ nhiệt động lực học. Mặc dù không được trực quan lắm, cách giải thích dựa trên năng lượng này thực sự chỉ là một ví dụ cho mô hình dựa trên năng lượng và phù hợp với khung lý thuyết về học dựa trên năng lượng mà nhiều thuật toán học có thể được thể hiện.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 640px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c7076fe67d8949d88ac7cb1dec0a6e49/c08c5/e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd5XIxh//8QAFxAAAwEAAAAAAAAAAAAAAAAAAAERIP/aAAgBAQABBQKCz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABoQAAICAwAAAAAAAAAAAAAAAAAQAREhQeH/2gAIAQEAAT8hEs9KW1//2gAMAwEAAgADAAAAEIAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxAAAgMAAwAAAAAAAAAAAAAAAREAECFBYXH/2gAIAQEAAT8QQPTvcYhPiZwJyaWa/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpeg\"\n        title=\"e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpeg\"\n        src=\"/static/c7076fe67d8949d88ac7cb1dec0a6e49/c08c5/e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpg\"\n        srcset=\"/static/c7076fe67d8949d88ac7cb1dec0a6e49/651be/e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpg 170w,\n/static/c7076fe67d8949d88ac7cb1dec0a6e49/d30a3/e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpg 340w,\n/static/c7076fe67d8949d88ac7cb1dec0a6e49/c08c5/e4147927defc7b9e4cfdfc17eb209d9f656abdd1.jpg 640w\"\n        sizes=\"(max-width: 640px) 100vw, 640px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một mô hình xác suất (còn được gọi là mô hình bayesian) đơn giản - một máy Boltzmann về cơ bản giống như trên nhưng kết hợp với các kết nối vô hướng/đối xứng và trọng số có thể huấn luyện để học  các xác suất theo một kiểu cụ thể (</em><em><a href=\"https://commons.wikimedia.org/wiki/File:SimpleBayesNet.svg\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Trở lại với máy Boltzmann. Khi các node như vậy được kết hợp với nhau thành một mạng, chúng tạo thành một đồ thị, cũng như là một mô hình trực quan của dữ liệu. Về cơ bản, chúng có thể làm những điều rất giống với mạng nơ-ron bình thường: một vài node ẩn tính xác suất của các biến ẩn (những đầu ra - phân loại hoặc đặc trưng của dữ liệu) dựa trên giá trị của node biểu diễn các biến hữu hình (những đầu vào - pixel ảnh, ký tự văn bản...). Trong ví dụ cổ điển của chúng tôi về phân loại ảnh chữ số, những biến ẩn là giá trị thật của các chữ số, và những biến hữu hình là các pixel ảnh. Ví dụ với đầu vào là ảnh của chữ số ”1”, ta đã biết giá trị của các node hữu hình, và node ẩn mô hình hóa xác suất của ảnh đại diện cho ”1” sẽ có xác suất đầu ra cao.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 433px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3a00d84d0256a41172412b7adad210b3/3561e/d0b4ff1a3a433248c971863a58ee6574de2300bd.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 94.70588235294117%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGQABAAIDAAAAAAAAAAAAAAAAAAECAwQF/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHr0y0udlCaAkH/xAAaEAACAgMAAAAAAAAAAAAAAAABAgAxEBEh/9oACAEBAAEFAnfu2AjYENCp/8QAFREBAQAAAAAAAAAAAAAAAAAAESD/2gAIAQMBAT8BI//EABURAQEAAAAAAAAAAAAAAAAAABEg/9oACAECAQE/AWP/xAAZEAEAAgMAAAAAAAAAAAAAAAABAAIhMEH/2gAIAQEABj8CadgGdH//xAAaEAEBAQEAAwAAAAAAAAAAAAABEQAxEGGB/9oACAEBAAE/ISQ0R3fdpcNwbdLLz3gTdMPD/9oADAMBAAIAAwAAABBECDz/xAAXEQEAAwAAAAAAAAAAAAAAAAARASAh/9oACAEDAQE/ENrNP//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QD//EAB4QAQADAAICAwAAAAAAAAAAAAEAESExURCBYbHB/9oACAEBAAE/EGRzhPf1L4iGmD+wBY3HtmGeupbIqq7BxwZBHlvdhuzqAHPnx//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"d0b4ff1a3a433248c971863a58ee6574de2300bd.jpeg\"\n        title=\"d0b4ff1a3a433248c971863a58ee6574de2300bd.jpeg\"\n        src=\"/static/3a00d84d0256a41172412b7adad210b3/3561e/d0b4ff1a3a433248c971863a58ee6574de2300bd.jpg\"\n        srcset=\"/static/3a00d84d0256a41172412b7adad210b3/651be/d0b4ff1a3a433248c971863a58ee6574de2300bd.jpg 170w,\n/static/3a00d84d0256a41172412b7adad210b3/d30a3/d0b4ff1a3a433248c971863a58ee6574de2300bd.jpg 340w,\n/static/3a00d84d0256a41172412b7adad210b3/3561e/d0b4ff1a3a433248c971863a58ee6574de2300bd.jpg 433w\"\n        sizes=\"(max-width: 433px) 100vw, 433px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một ví dụ của máy Boltzmann. Mỗi dòng có một trọng số liên kết, như với mạng nơ-ron. Lưu ý rằng không có khái niệm về lớp ở đây, mọi thứ có thể được kết nối với nhau. Chúng ta sẽ nói về biến thể này trên mạng nơ-ron sau. (</em><em><a href=\"https://en.wikipedia.org/wiki/File:Boltzmannexamplev1.png\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Vì vậy, đối với nhiệm vụ phân loại, giờ đây đã có một cách hay để tính xác suất của từng loại. Điều này rất giống với việc thực sự tính giá trị đầu ra của một mạng nơ-ron phân loại thông thường, nhưng những mạng này có một điểm hay khác: chúng có thể tạo ra dữ liệu đầu vào một cách hợp lý. Bởi theo như các phương trình xác suất có liên quan - mạng không chỉ học giá trị xác suất của các biến ẩn từ giá trị đã biết của các biến hữu hình, mà còn ở chiều ngược lại, những xác suất hữu hình từ các giá trị ẩn. Vì vậy, nếu chúng ta muốn tạo ra ảnh chữ số ”1”, các node tương ứng với các biến pixel có xác suất xuất ra 1 đã được biết, và hình ảnh có thể được tạo theo xác suất đó. Những mạng này là mô hình sinh ảnh (generative graphical models). Mặc dù có thể thực hiện việc học có giám sát cho các mục đích tương tự như với mạng nơ-ron bình thường, những tác vụ học không giám sát như học một mô hình sinh tốt hay học theo xác suất cấu trúc ẩn của dữ liệu thường là những gì mạng này được dùng cho. Hầu hết những điều này thật sự không mới lạ, nhưng thuật toán học được trình bày và công thức cụ thể đã hiện thực nó là một điểm mới, như được nêu trong chính bài báo:</p>\n<p>Nếu không đi sâu vào chi tiết thuật toán thì đây là một số điểm nổi bật: nó là một biến thể của những thuật toán hợp lý cực đại (maximum-likelihood), hiểu một cách đơn giản thì nó tìm cách tối đa hóa xác suất của những giá trị node hữu hình (visible) trong mạng khớp với những giá trị chính xác đã biết. Việc tính giá trị thực tế có khả năng xảy ra cao nhất cho tất cả các node cùng lúc hóa ra lại quá tốn chi phí, do đó trong huấn luyện Gibbs Sampling (lấy mẫu Gibbs) - bắt đầu mạng với các giá trị node ngẫu nhiên và gán lại giá trị cho các node dựa trên dựa trên giá trị kết nối của chúng trong từng vòng lặp - được dùng để cung cấp một số giá trị thực đã biết. Khi học bằng cách sử dụng một tập huấn luyện, các node hữu hình được gán dựa trên giá trị của mẫu huấn luyện hiện tại, vì vậy việc lấy mẫu được thực hiện là để tìm giá trị cho các node ẩn. Cụ thể, khi một số giá trị ”thực” được lấy mẫu, chúng ta có thể làm điều tương tự như với lan truyền ngược - lấy đạo hàm cho mỗi trọng số để xem chúng ta có thể thay đổi thế nào để tăng xác suất thực hiện đúng của mạng.</p>\n<p>Như với mạng nơ-ron, thuật toán có thể được thực hiện theo kiểu có giám sát (với những giá trị đã biết cho các node ẩn) hoặc theo kiểu không giám sát. Mặc dù thuật toán đã được chứng minh hoạt động được (đáng chú ý là với cùng bài toán mã hóa mà autoencoder giải quyết) nhưng đã sớm thấy rõ là nó không hoạt động tốt lắm - bài báo “Connectionist learning of belief networks” (Học kết nối của các mô hình xác suất) của Redford M. Neal xuất bản năm 1992 đã chứng minh nhu cầu về một cách tiếp cận nhanh hơn bằng cách chỉ ra rằng: “Những khả năng này sẽ làm cho máy Boltzmann trở nên hấp dẫn trong nhiều ứng dụng, nếu quy trình học của nó không thường bị xem là chậm một cách khó chấp nhận”. Và vì vậy Neal đã đưa ra một ý tưởng tương tự trong mô hình xác suất, về cơ bản giống như một máy Boltzmann với các kết nối truyền xuôi và có hướng (thế nên lại có các lớp, như với mạng nơ-ron trước đó, và không giống với máy Boltzmann vừa nêu ở trên). Với việc không liên quan gì đến toán xác suất, thay đổi này cho phép mạng được huấn luyện với thuật toán học nhanh hơn. Chúng ta đã thực sự thấy một mô hình xác suất phía trên với ”bình tưới” và những ”giọt nước”, và thuật ngữ này (belief net) được lựa chọn một cách chính xác bởi vì loại mô hình dựa trên xác suất này có mối quan hệ chặt chẽ với các ý tưởng từ toán xác suất, bên cạnh mối quan hệ với học máy.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/da4883222ba14945a9d1581bd19a46a2/80e3c/4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.29411764705883%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe2yigf/xAAZEAEAAgMAAAAAAAAAAAAAAAAAAREhMUH/2gAIAQEAAQUCtpDuUP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABsQAAMBAAMBAAAAAAAAAAAAAAABESFBUWFx/9oACAEBAAE/IW9aL2+2jVbL4NOsOGFTT//aAAwDAQACAAMAAAAQ8A//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAAIDAQAAAAAAAAAAAAABEQAhMUFRof/aAAgBAQABPxDYobhBwCPByLnj7jJSDHw4kgte8QMVCS6wCKt7c//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpeg\"\n        title=\"4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpeg\"\n        src=\"/static/da4883222ba14945a9d1581bd19a46a2/7bf67/4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpg\"\n        srcset=\"/static/da4883222ba14945a9d1581bd19a46a2/651be/4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpg 170w,\n/static/da4883222ba14945a9d1581bd19a46a2/d30a3/4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpg 340w,\n/static/da4883222ba14945a9d1581bd19a46a2/7bf67/4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpg 680w,\n/static/da4883222ba14945a9d1581bd19a46a2/80e3c/4c1e5b671dd3ceed0ec56ae907438cfd1b2ae034.jpg 720w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một diễn giải về mô hình xác suất (</em><em><a href=\"http://www.slideserve.com/Leo/restricted-boltzmann-machines-and-deep-belief-networks\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Mặc dù cách tiếp cận này là một bước tiến so với máy Boltzmann, nó vẫn còn quá chậm - phép toán để xác định chính xác các mối quan hệ xác suất giữa các biến có chi phí tính toán cao, lại không có thủ thuật nào để đơn giản hóa. Vì vậy, Hinton cùng Neal và hai đồng tác giả khác, đã sớm đưa ra các thủ thuật bổ sung trong “The wake-sleep algorithm for unsupervised neural networks” (Thuật toán thức-ngủ cho các mạng nơ-ron không giám sát), xuất bản năm 1995. Những thủ thuật này có thiết lập hơi khác so với mô hình xác suất (belief net), được gọi là máy Helmholtz. Ý tưởng chính là có các bộ trọng số riêng biệt để suy ra các biến ẩn từ các biến hữu hình (trọng số nhận dạng) và ngược lại (trọng số sinh), và giữ nguyên khía cạnh có hướng ở mô hình xác suất của Neal. Điều này cho phép việc huấn luyện diễn ra nhanh hơn nhiều, đồng thời có thể áp dụng cho các bài toán học có giám sát và không giám sát của máy Boltzmann.</p>\n<p>Sau cùng, mô hình xác suất đã có thể được huấn luyện nhanh chóng. Dù không là một ảnh hưởng toàn diện, sự tiến bộ của thuật toán này là một bước tiến đủ quan trọng cho việc huấn luyện không giám sát các mô hình xác suất - có thể xem là người bạn đồng hành với ấn phẩm gần một thập kỷ nay về lan truyền ngược. Dẫu vậy, đến thời điểm này, các phương pháp học máy mới cũng bắt đầu xuất hiện và mọi người lại bắt đầu hoài nghi về mạng nơ-ron vì chúng dường như dựa trên trực giác và máy tính vẫn chưa thể đáp ứng được nhu cầu tính toán của chúng. Như chúng ta sẽ thấy ở phần sau, một ”mùa đông AI” mới của mạng nơ-ron đã bắt đầu chỉ vài năm sau...</p>\n<h2>Học cách đưa ra quyết định</h2>\n<p>Sau khi khám phá ra ứng dụng của mạng nơ-ron trong học không giám sát, hãy cũng xem qua chúng được sử dụng như thế nào trong nhánh thứ ba của học máy: học tăng cường (reinforcement learning). Điều này đòi hỏi nhiều định nghĩa toán học để giải thích rõ ràng, chính thống, một cách dễ hiểu thì học tăng cường được dùng để: học cách đưa ra quyết định đúng đắn. Cho một vài tác nhân (agent) trên lý thuyết (ví dụ: một chương trình phần mềm nhỏ), ý tưởng là làm cho agent có khả năng quyết định một hành động (action) dựa trên trạng thái (state) hiện tại của nó, dựa vào phần thưởng (reward) nhận được cho mỗi action và mục tiêu tối đa hóa lợi ích về lâu dài. Vì vậy, trong khi học có giám sát cho thuật toán biết chính xác những gì nó nên học để xuất ra thì học tăng cường dựa vào cơ chế ‘phần thưởng’ (mục đích ngắn hạn) để việc đưa ra quyết định tốt theo thời gian, và không trực tiếp cho thuật toán biết những quyết định chính xác để lựa chọn. Ngay từ đầu, nó đã là một mô hình ra quyết định khá trừu tượng - có một số lượng hữu hạn các trạng thái, và tập các hành động đã biết trước đi cùng với phần thưởng đã biết cho mỗi trạng thái. Điều này giúp ta dễ dàng viết các phương trình tìm tập các hành động tối ưu, nhưng khó áp dụng cho bài toán thực tế - các bài toán có số lượng trạng thái liên tục và khó để xác định phần thưởng cho từng  hành động ở mỗi trạng thái.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 437px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/76663048c26c0a38b6a7d54c00e86e25/fca8a/53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.35294117647058%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe9ENoP/xAAXEAADAQAAAAAAAAAAAAAAAAAAARAR/9oACAEBAAEFAqzZ/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGBAAAwEBAAAAAAAAAAAAAAAAAAEhEBH/2gAIAQEAAT8hYnDpGEf/2gAMAwEAAgADAAAAECDP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHBAAAgICAwAAAAAAAAAAAAAAASEAEUFRMWFx/9oACAEBAAE/ECJKF+wBKmdalMxF9wsFzeYCeZ//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpeg\"\n        title=\"53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpeg\"\n        src=\"/static/76663048c26c0a38b6a7d54c00e86e25/fca8a/53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpg\"\n        srcset=\"/static/76663048c26c0a38b6a7d54c00e86e25/651be/53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpg 170w,\n/static/76663048c26c0a38b6a7d54c00e86e25/d30a3/53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpg 340w,\n/static/76663048c26c0a38b6a7d54c00e86e25/fca8a/53f06dbbb88f1532b225bfaaa1cafd747d5a11a1.jpg 437w\"\n        sizes=\"(max-width: 437px) 100vw, 437px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Học tăng cường (</em><em><a href=\"http://www2.hawaii.edu/~chenx/ics699rl/grid/rl.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Đây là lúc mạng nơ-ron xuất hiện. Học máy nói chung và mạng thần kinh nói riêng, rất tốt trong việc xử lý dữ liệu liên tục hỗn độn hoặc các hàm khó định nghĩa bằng cách học chúng từ các mẫu dữ liệu có sẵn. Mặc dù phân loại là phần quan trọng của các mạng nơ-ron, nhưng chúng cũng hữu ích cho nhiều loại bài toán khác (như dùng cho bộ lọc thích ứng trong các mạch điện). Do đó, theo sau sự hồi sinh của các nghiên cứu nhờ thuật toán lan truyền ngược, mọi người đã sớm nghĩ ra cách tận dụng sức mạnh của mạng nơ-ron để thực hiện việc học tăng cường. Một trong những ví dụ đầu của việc này là giải một bài toán kinh điển nhưng đơn giản: sự cân bằng của một chiếc gậy trên một bệ chuyển động, được học sinh trong các lớp điều khiển ở khắp mọi nơi gọi là bài toán con lắc ngược.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/86b10cc21b77736105e706362fd1aa7e/212bf/47e6bf12ed36422c10df02d3d7d734d952440b27.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.35294117647059%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBv/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAegiGkH/xAAXEAADAQAAAAAAAAAAAAAAAAABEBEg/9oACAEBAAEFAsQL/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGRAAAgMBAAAAAAAAAAAAAAAAEBEAASFB/9oACAEBAAE/IZ06NUx//9oADAMBAAIAAwAAABBwz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQACAwEAAAAAAAAAAAAAAAEQEQAhMUH/2gAIAQEAAT8Qy+JCZJe1uP/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"47e6bf12ed36422c10df02d3d7d734d952440b27.jpeg\"\n        title=\"47e6bf12ed36422c10df02d3d7d734d952440b27.jpeg\"\n        src=\"/static/86b10cc21b77736105e706362fd1aa7e/7bf67/47e6bf12ed36422c10df02d3d7d734d952440b27.jpg\"\n        srcset=\"/static/86b10cc21b77736105e706362fd1aa7e/651be/47e6bf12ed36422c10df02d3d7d734d952440b27.jpg 170w,\n/static/86b10cc21b77736105e706362fd1aa7e/d30a3/47e6bf12ed36422c10df02d3d7d734d952440b27.jpg 340w,\n/static/86b10cc21b77736105e706362fd1aa7e/7bf67/47e6bf12ed36422c10df02d3d7d734d952440b27.jpg 680w,\n/static/86b10cc21b77736105e706362fd1aa7e/212bf/47e6bf12ed36422c10df02d3d7d734d952440b27.jpg 768w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Bài toán điều khiển con lắc đôi - một bước tiến so với phiên bản con lắc đơn, đây là một tác vụ điều khiển và học tăng cường kinh điển.</em></p>\n<p>Như với lọc thích ứng, nghiên cứu này có liên quan chặt chẽ đến Kỹ thuật Điện, nơi lý thuyết điều khiển đã là một lĩnh vực con chính trong nhiều thập kỷ trước khi xuất hiện mạng nơ-ron. Mặc dù lĩnh vực này đã nghĩ ra cách để đối phó với nhiều bài toán thông qua phân tích trực tiếp, nhưng việc có một phương tiện cho các tình huống phức tạp hơn thông qua việc học tỏ ra hữu ích, như được chứng minh bằng 7000 trích dẫn của bài báo năm 1990 “Identification and control of dynamical systems using neural networks” (Xác định và điều khiển hệ thống động lực sử dụng mạng nơ-ron): Chắc là có thể đoán trước được, có một lĩnh vực khác tách biệt với học máy, nơi mạng nơ-ron tỏ ra hữu ích - robotics (robot học). Một ví dụ điển hình của việc sử dụng mạng nơ-ron cho robotics những ngày đầu đến từ NavLab (phòng thí nghiệm thuộc CMU) với bài báo năm 1989 “Alvinn: An autonomous land vehicle in a neural network” (Alvinn: Một phương tiện tự hành trong mạng nơ-ron).</p>\n<p><div class=\"gatsby-resp-iframe-wrapper\" style=\"padding-bottom: 56.42857142857143%; position: relative; height: 0; overflow: hidden; \" > <div class=\"embedVideo-container\"> <iframe title src=\"https://www.youtube.com/embed/5-acCtyKf7E?rel=0\" class=\"embedVideo-iframe\" style=\"border:0; position: absolute; top: 0; left: 0; width: 100%; height: 100%; \" allowfullscreen></iframe> </div> </div></p>\n<p>Như đã thảo luận trong bài báo, mạng nơ-ron trong hệ thống này học cách điều khiển phương tiện thông qua việc học có giám sát thuần túy bằng cách sử dụng cảm biến và dữ liệu lái được ghi lại khi con người lái xe. Cũng đã có nghiên cứu về việc dạy robot sử dụng phương pháp học tăng cường một cách đặc biệt, như trong một luận án tiến sĩ năm 1993 “Reinforcement learning for robots using neural networks” (Học tăng cường cho robt sử dụng mạng nơ-ron). Luận án này cho thấy robot có thể được dạy các hành vi như theo sát tường và đi qua cửa trong lượng thời gian hợp lý, một điểm tốt khi xét đến bài toán con lắc ngược trước đây - đòi hỏi thời gian đào tạo không thực tế.</p>\n<p>Sự đa dạng các ứng dụng trong nhiều lĩnh vực khác chắc chắn rất thú vị, nhưng tất nhiên hầu hết các nghiên cứu về học tăng cường và mạng nơ-ron đều đang diễn ra trong lĩnh vực trí tuệ nhân tạo (AI) và học máy. Và ở đây, ta đã đạt được một trong những kết quả quan trọng nhất trong lịch sử của học tăng cường: một mạng nơ-ron học để trở thành một kỳ thủ Backgammon (cờ tào cáo) đẳng cấp thế giới. Được đặt tên là TD-Gammon, mạng nơ-ron này được huấn luyện bằng cách sử dụng thuật toán học tăng cường tiêu chuẩn và là một trong những minh chứng đầu tiên về việc học tăng cường có thể hoạt động tốt hơn con người trong các nhiệm vụ tương đối phức tạp. Đặc biệt hơn cả là hướng tiếp cận học tăng cường đã giải quyết được bài toán trên, vì nghiên cứu tương tự cho thấy chỉ sử dụng mạng nơ-ron mà không học tăng cường gần như không hoạt động.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7dd7c90a92dbf6dc789924aacc650262/41099/64c960f0a369209b24d6d362ab671b621d18b15a.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.764705882352935%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHdSiWjD//EABcQAAMBAAAAAAAAAAAAAAAAAAABEAL/2gAIAQEAAQUCYpq//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAHBAAAgICAwAAAAAAAAAAAAAAAAERIRAxYYGR/9oACAEBAAE/IaK76IpUvCeGboW8f//aAAwDAQACAAMAAAAQ48//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAeEAEAAgEEAwAAAAAAAAAAAAABABFBECExUXHw8f/aAAgBAQABPxBmGfKKpc5RzPcRNF7dRoKU+6f/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"64c960f0a369209b24d6d362ab671b621d18b15a.jpeg\"\n        title=\"64c960f0a369209b24d6d362ab671b621d18b15a.jpeg\"\n        src=\"/static/7dd7c90a92dbf6dc789924aacc650262/41099/64c960f0a369209b24d6d362ab671b621d18b15a.jpg\"\n        srcset=\"/static/7dd7c90a92dbf6dc789924aacc650262/651be/64c960f0a369209b24d6d362ab671b621d18b15a.jpg 170w,\n/static/7dd7c90a92dbf6dc789924aacc650262/d30a3/64c960f0a369209b24d6d362ab671b621d18b15a.jpg 340w,\n/static/7dd7c90a92dbf6dc789924aacc650262/41099/64c960f0a369209b24d6d362ab671b621d18b15a.jpg 500w\"\n        sizes=\"(max-width: 500px) 100vw, 500px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Mạng nơ-ron học cách chơi Backgammon đẳng cấp kỳ thủ.</em></p>\n<p>Nhưng, như chúng ta đã thấy trước đây và sẽ thấy lần nữa trong AI, nghiên cứu đã đi vào ngõ cụt. Bài toán tiếp theo được dự đoán là có thể giải quyết bằng cách sử dụng phương pháp TD-Gammon đã được Sebastian Thrun nghiên cứu trong “Learning To Play the Game of Chess” (Học chơi cờ vua), năm 1995, cho kết quả không tốt. Mặc dù mạng nơ-ron đã học được cách chơi tốt, chắc chắn tốt hơn so với một người mới hoàn toàn, nhưng nó vẫn kém hơn nhiều so với một chương trình máy tính tiêu chuẩn (GNU-Chess) được triển khai từ rất lâu trước đó. Điều tương tự xảy ra với thử thách lâu năm khác của AI, Go (cờ vây). Hãy xem, cách chơi của TD-Gammon khá hạn hẹp - nó học cách đánh giá vị trí tốt, nên có thể đi tiếp mà không thực hiện bất kỳ “tìm kiếm“ nào cho một loạt nhiều nước đi trong tương lai, nó chỉ chọn nước đi dẫn đến vị trí tiếp theo tốt nhất. Nhưng điều này đơn giản là không thể xảy ra trong cờ vua hoặc cờ vây, những trò chơi thách thức AI vì cần có tầm nhìn xa hơn, cho nhiều nước đi trong tương lai, và có rất nhiều khả năng kết hợp các nước đi với nhau. Bên cạnh đó, ngay cả khi thuật toán đã thông minh hơn, phần cứng ở thời điểm đó vẫn không đáp ứng được yêu cầu - Thrun báo cáo rằng “NeuroChess (các agent chơi cờ) chơi khá kém, vì nó dành phần lớn thời gian để đánh giá bàn cờ. Tính toán một hàm dùng mạng nơ-ron mất chi phí lớn hơn hai bậc so với một hàm tuyến tính được tối ưu hóa (như là GNU-Chess)“. Sự yếu kém của máy tính thời đó so với nhu cầu của mạng nơ-ron là một vấn đề rất thực tế, và như chúng ta sẽ thấy nó không phải là vấn đề duy nhất…</p>\n<h2>Những vòng lặp có phần ngớ ngẩn</h2>\n<p>Dù học không  giám sát và học tăng cường có vẻ hay ho, tôi nghĩ học có giám sát vẫn là lựa chọn yêu thích của tôi khi nhắc đến mạng nơ-ron. Hẳn là việc học các mô hình dữ liệu theo xác suất thật thú vị, nhưng thú vị hơn nhiều khi các vấn đề cụ thể được giải quyết bằng lan truyền ngược. Chúng ta đã thấy cách Yann Lecun đạt được thành tựu khá tốt với nhận dạng chữ số viết tay (một công nghệ đã được triển khai trên toàn nước Mỹ cho việc đọc-kiểm và hơn thế nữa sau đó), nhưng có một nhiệm vụ quan trọng khác cũng đang được thực hiện vào thời điểm đó: hiểu lời nói của con người.</p>\n<p>Như với chữ viết, khá khó khăn để hiểu được lời nói của con người do thực tế có muôn hình vạn trạng cách một từ được nói. Tuy nhiên, có một thách thức nữa ở đây: những chuỗi đầu vào dài. Với hình ảnh, đơn giản chỉ là cắt ra một chữ cái từ một ảnh và có một mạng nơ-ron cho bạn biết đó là chữ cái nào, theo kiểu đầu vào → đầu ra. Nhưng với audio, chuyện không đơn giản thế - tách lời nói thành các ký tự là một việc phi thực tế, thậm chí việc tìm các từ riêng lẻ trong lời nói cũng chẳng đơn giản hơn. Thêm vào đó, lời nói của con người nhìn chung sẽ dễ hiểu hơn khi đặt vào một ngữ cảnh cụ thể hơn là bị tách rời. Mặc dù cấu trúc này hoạt động khá tốt để xử lý từng thứ một như hình ảnh, theo kiểu đầu vào → đầu ra, nó không hoàn toàn phù hợp với các luồng thông tin dài như âm thanh hoặc văn bản. Mạng nơ-ron không có ‘bộ nhớ’ để một đầu vào có thể ảnh hưởng đến đầu vào khác được xử lý sau đó, nhưng đây chính xác là cách con người chúng ta xử lý âm thanh hoặc văn bản - một chuỗi các từ hoặc âm thanh, thay vì một tập các đầu vào đơn lẻ. Mấu chốt ở đây là: để giải quyết bài toán hiểu được lời nói của con người, các nhà nghiên cứu đã tìm cách sửa đổi mạng mạng nơ-ron để xử lý đầu vào dưới dạng một luồng dữ liệu như trong lời nói chứ không phải một tập rời rạc như với hình ảnh.</p>\n<p>Alexander Waibel và các cộng sự (bao gồm Hinton) có một cách tiếp cận cho vấn đề này, được giới thiệu năm 1989 “Phoneme recognition using time-delay neural networks” (Nhận dạng âm vị bằng cách sử dụng mạng nơron với độ trễ thời gian). Các mạng nơ-ron có độ trễ thời gian (TDNN) này rất giống với mạng nơ-ron thông thường, ngoại trừ mỗi nơ-ron chỉ xử lý một tập con của đầu vào và có một số bộ trọng số cho các độ trễ khác nhau của dữ liệu đầu vào. Nói cách khác, với một chuỗi đầu vào âm thanh, một ‘cửa sổ trượt‘ của âm thanh được đưa vào mạng và khi cửa sổ di chuyển, các bit âm thanh sẽ được xử lý bởi mỗi nơ-ron với các bộ trọng số khác nhau dựa trên vị trí của chúng trong cửa sổ trượt. Có thể hiểu rõ hơn qua một minh họa nhanh dưới đây:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 477px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a9396c9a71922e767b781d7188e8a6d8/b18ba/cdf545025ac6427af2de31cd419a125ef04d90d2.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.64705882352942%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHflpKAX//EABcQAAMBAAAAAAAAAAAAAAAAAAABEBH/2gAIAQEAAQUCNqn/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAACAwEAAAAAAAAAAAAAAAABMQAQEUH/2gAIAQEAAT8hTMGjRU4r/9oADAMBAAIAAwAAABBvz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAAICAgMAAAAAAAAAAAAAAAABESExYVFxwf/aAAgBAQABPxBUyPY5qiFsmcEm9X2LC9F8n//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"cdf545025ac6427af2de31cd419a125ef04d90d2.jpeg\"\n        title=\"cdf545025ac6427af2de31cd419a125ef04d90d2.jpeg\"\n        src=\"/static/a9396c9a71922e767b781d7188e8a6d8/b18ba/cdf545025ac6427af2de31cd419a125ef04d90d2.jpg\"\n        srcset=\"/static/a9396c9a71922e767b781d7188e8a6d8/651be/cdf545025ac6427af2de31cd419a125ef04d90d2.jpg 170w,\n/static/a9396c9a71922e767b781d7188e8a6d8/d30a3/cdf545025ac6427af2de31cd419a125ef04d90d2.jpg 340w,\n/static/a9396c9a71922e767b781d7188e8a6d8/b18ba/cdf545025ac6427af2de31cd419a125ef04d90d2.jpg 477w\"\n        sizes=\"(max-width: 477px) 100vw, 477px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Mạng nơron với độ trễ thời gian (</em><em><a href=\"https://electroviees.wordpress.com/tag/chacha/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Theo một nghĩa nào đó, điều này khá giống với những gì CNN làm - thay vì xử lý toàn bộ dữ liệu đầu vào cùng lúc, mỗi node chỉ xem xét một tập hợp con tại một thời điểm và thực hiện tính toán tương tự cho từng tập hợp con. Sự khác biệt chính ở đây là không có ý niệm về thời gian trong CNN và ‘cửa sổ‘ đầu vào cho mỗi node luôn được di chuyển trên toàn bộ ảnh để tính toán kết quả, trong khi TDNN thực sự là chuỗi đầu vào và đầu ra của dữ liệu. Có một sự thật thú vị: theo như Hinton, ý tưởng về TDNN là thứ đã truyền cảm hứng cho LeCun phát triển mạng nơ-ron tích chập. Dẫu vậy, CNN đã trở nên cần thiết cho việc xử lý hình ảnh, trong khi với nhận dạng giọng nói, TDNN đã bị vượt qua bởi một cách tiếp cận khác - mạng nơ-ron hồi tiếp (RNN). Tất cả các mạng đã được thảo luận cho đến nay đều là mạng truyền xuôi - đầu ra của các nơ-ron trong một lớp nhất định đóng vai trò là đầu vào chỉ cho các nơ-ron ở lớp tiếp theo. Nhưng, không nhất thiết phải như vậy - không có gì cấm các nhà khoa học máy tính dũng cảm dùng đầu ra của lớp cuối cùng như một đầu vào cho lớp đầu tiên, hoặc dùng đầu ra của một nơ-ron như đầu vào cho chính nó. Bằng cách cho đầu ra của mạng ‘vòng lặp‘ trở lại, việc làm cho mạng có khả năng nhớ trước đây đã được giải quyết một cách rất dễ dàng!</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 400px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca05f53e88756ab6a00339677ce32482/066f9/e212b25b29ef1375a69edc1291a8669c57b92680.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.41176470588235%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAQAF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdhGGqv/xAAYEAEBAQEBAAAAAAAAAAAAAAABABIQEf/aAAgBAQABBQIht8D2xf/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABEDH/2gAIAQEABj8Cdc0//8QAGRABAQADAQAAAAAAAAAAAAAAAQAQETFh/9oACAEBAAE/IVoiheiORusH/9oADAMBAAIAAwAAABDvz//EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAEDAQE/EGf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAdEAEAAwABBQAAAAAAAAAAAAABABEhMUFRYbHB/9oACAEBAAE/EEovT7Hlm5Q95QxF8QqZvHuDJhKdMil5T//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"e212b25b29ef1375a69edc1291a8669c57b92680.jpeg\"\n        title=\"e212b25b29ef1375a69edc1291a8669c57b92680.jpeg\"\n        src=\"/static/ca05f53e88756ab6a00339677ce32482/066f9/e212b25b29ef1375a69edc1291a8669c57b92680.jpg\"\n        srcset=\"/static/ca05f53e88756ab6a00339677ce32482/651be/e212b25b29ef1375a69edc1291a8669c57b92680.jpg 170w,\n/static/ca05f53e88756ab6a00339677ce32482/d30a3/e212b25b29ef1375a69edc1291a8669c57b92680.jpg 340w,\n/static/ca05f53e88756ab6a00339677ce32482/066f9/e212b25b29ef1375a69edc1291a8669c57b92680.jpg 400w\"\n        sizes=\"(max-width: 400px) 100vw, 400px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Sơ đồ một mạng nơ-ron hồi tiếp. Nó có gợi nhớ đến máy Boltzmann trước đó? Chúng chính là những mạng nơ-ron hồi tiếp. (</em><em><a href=\"http://www.wolframalpha.com/docs/timeline/computable-knowledge-history-6.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Nhưng nó không hoàn toàn đơn giản thế. Nếu thuật toán lan truyền ngược dựa vào việc ‘lan truyền’ lỗi từ lớp đầu ra về các lớp trước đó, thì mọi thứ hoạt động thế nào nếu lớp đầu tiên kết nối trở lại với lớp cuối cùng? Lỗi sẽ được lan truyền từ lớp đầu tiên về lớp cuối và có thể tiếp tục lặp lại vô hạn quá trình này trong mạng. Giải pháp, được nghiên cứu độc lập bởi nhiều nhóm, là sự lan truyền ngược liên hồi (Backpropagation Through Time). Về cơ bản, ý tưởng là ‘bỏ cuộn‘ mạng nơ-ron hồi tiếp bằng cách coi mỗi vòng lặp qua mạng như một đầu vào cho một mạng nơ-ron khác và chỉ lặp lại một số lần giới hạn.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f8fb1974a93f4c9fec88e8f83db15530/b52a8/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 32.94117647058823%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3pCgf//EABUQAQEAAAAAAAAAAAAAAAAAABAR/9oACAEBAAEFAq//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAQ/9oACAEBAAY/An//xAAXEAADAQAAAAAAAAAAAAAAAAAAARFx/9oACAEBAAE/IRHRYf/aAAwDAQACAAMAAAAQcA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAaEAACAwEBAAAAAAAAAAAAAAAAAREhkUHw/9oACAEBAAE/EI8TFYj2iKKfT//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpeg\"\n        title=\"2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpeg\"\n        src=\"/static/f8fb1974a93f4c9fec88e8f83db15530/7bf67/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg\"\n        srcset=\"/static/f8fb1974a93f4c9fec88e8f83db15530/651be/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg 170w,\n/static/f8fb1974a93f4c9fec88e8f83db15530/d30a3/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg 340w,\n/static/f8fb1974a93f4c9fec88e8f83db15530/7bf67/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg 680w,\n/static/f8fb1974a93f4c9fec88e8f83db15530/990cb/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg 1020w,\n/static/f8fb1974a93f4c9fec88e8f83db15530/b52a8/2b7e3b8e41ca260d01f4254d6e51b66c072a2876.jpg 1267w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một cái nhìn trực quan về khái niệm lan truyền ngược liên hồi (</em><em><a href=\"https://upload.wikimedia.org/wikipedia/en/e/ee/Unfold_through_time.png\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Ý tưởng khá đơn giản này thực sự đã hoạt động - nó có thể huấn luyện các mạng nơ-ron hồi tiếp. Và thực sự, nhiều người đã khám phá ứng dụng của RNN cho nhận dạng giọng nói. Tuy nhiên, đây là một ‘cú xoắn‘ mà bạn có thể đoán được: cách tiếp cận này không hoạt động tốt. Để hiểu vì sao, hãy cùng gặp một gã khổng lồ hiện đại khác của học sâu: Yoshua Bengio. Bắt đầu nghiên cứu về nhận dạng giọng nói với mạng nơ-ron vào khoảng năm 1986, ông là đồng tác giả của nhiều bài báo về sử dụng ANN và RNN để nhận dạng giọng nói, và cuối cùng làm việc tại Bell Labs về bài toán này giống như Yann LeCun đang làm về CNN ở đó. Trên thực tế, vào năm 1995, họ đã cùng viết bài báo tóm tắt “Convolutional Networks for Images, Speech, and Time-Series” (Mạng tích chập cho ảnh,  lời nói và dữ liệu thời gian), bài báo đầu tiên trong số nhiều sự hợp tác giữa họ. Tuy nhiên, trước đó - vào năm 1993, Bengio đã viết về “A Connectionist Approach to Speech Recognition” (Một phương pháp tiếp cận dựa trên kết nối để nhận dạng giọng nói). Đây là tóm tắt của ông về thất bại chung của việc giảng dạy hiệu quả RNN:</p>\n<h2>Bắt đầu có khả năng nói</h2>\n<p>Về Bengio, những đóng góp của ông đối với mạng nơ-ron còn hơn cả mong đợi với CNN và RNN. Đặc biệt, khi nhắc đến điều này, không thể không kể đến bài báo năm 2003 của ông: “A Neural Probabilistic Language Model” (Mô hình ngôn ngữ xác suất nơ-ron). Như tiêu đề ngụ ý, công trình này liên quan đến việc sử dụng mạng nơ-ron để mô hình hóa ngôn ngữ - một nhiệm vụ cơ bản trong lĩnh vực Xử lý ngôn ngữ tự nhiên (NLP), tập trung vào việc dự đoán những từ sẽ xuất hiện tiếp theo dựa vào một số từ trước nó (về cơ bản là tự động điền). Nhiệm vụ này sau đó đã được nghiên cứu trong một thời gian dài, với các phương pháp kinh điển như đếm tần suất các từ riêng lẻ và các cụm từ khác nhau (hay còn gọi là n-grams) xuất hiện trong một nhóm văn bản, sau đó sử dụng kết quả này để ước tính xác suất từ sẽ xuất hiện tiếp theo dựa trên những từ trước nó:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2929db0a35551aa77883dee863337f06/dc4d4/0a6b5435511821962656aa61823b5e01127699a3.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.29411764705882%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3gUH/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAFxABAQEBAAAAAAAAAAAAAAAAAAERIf/aAAgBAQABPyG2ONf/2gAMAwEAAgADAAAAEIAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAFxABAQEBAAAAAAAAAAAAAAAAAQARgf/aAAgBAQABPxDoLQ7sBv/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"0a6b5435511821962656aa61823b5e01127699a3.jpeg\"\n        title=\"0a6b5435511821962656aa61823b5e01127699a3.jpeg\"\n        src=\"/static/2929db0a35551aa77883dee863337f06/7bf67/0a6b5435511821962656aa61823b5e01127699a3.jpg\"\n        srcset=\"/static/2929db0a35551aa77883dee863337f06/651be/0a6b5435511821962656aa61823b5e01127699a3.jpg 170w,\n/static/2929db0a35551aa77883dee863337f06/d30a3/0a6b5435511821962656aa61823b5e01127699a3.jpg 340w,\n/static/2929db0a35551aa77883dee863337f06/7bf67/0a6b5435511821962656aa61823b5e01127699a3.jpg 680w,\n/static/2929db0a35551aa77883dee863337f06/dc4d4/0a6b5435511821962656aa61823b5e01127699a3.jpg 795w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một minh họa trực quan của khái niệm lan truyền ngược liên hồi (</em><em><a href=\"https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Mặc dù cách tiếp cận này rất thành công, nhưng nó cũng có những hạn chế nhất định, vì nó yêu cầu phải nhìn thấy một từ hoặc nhiều từ trước đó để dự đoán từ tiếp theo, mà ngôn ngữ của con người  thì có một số lượng lớn các từ với nhiều sự kết hợp có nghĩa theo cấp số nhân. Thêm vào đó, con người chúng ta không chỉ lý luận về ngôn ngữ dựa trên những gì chúng ta đã thấy mà còn cả những gì chúng ta biết về ý nghĩa của từng từ; nếu chúng ta chỉ thấy “I have a pet dog” và sau đó học được rằng có tồn tại một từ - chẳng hạn như “cat” - cũng là loài thú cưng phổ biến, thì chẳng có khó khăn gì để suy tưởng rằng “I have a pet cat” là điều mà người khác cũng có khả năng sẽ nói.</p>\n<p>Vì vậy, làm thế nào để máy tính hiểu được các từ khác nhau có ngữ nghĩa giống nhau ra sao? Xét cho cùng, ý nghĩa của các từ khá tinh tế, nên việc thực hiện một điều đơn giản như so sánh các định nghĩa của từ dựa trên cấu tạo của chúng thì không có khả năng hoạt động tốt. Và rồi, hóa ra sự tinh tế của ngữ nghĩa có thể được nắm bắt một cách độc đáo trong một định dạng dễ dàng hơn để đánh giá sự giống nhau: danh sách các con số. Giống như việc có thể tính khoảng cách của hai điểm bất kỳ trong không gian chiều, hai điểm bất kỳ trong không gian 100 chiều cũng có khoảng cách của riêng nó, và có thể nói rằng những điểm gần nhau thì tương tự nhau hơn là những điểm ở cách xa nhau. Do đó, nếu chúng ta có thể ánh xạ mọi từ đến một điểm thích hợp trong không gian nhiều chiều, sao cho các từ tương tự nhau được ánh xạ tới các điểm gần nhau, thì sẽ khá hữu ích cho việc mô hình hóa ngôn ngữ. Hinrich Schütze đã giới thiệu ý tưởng này trong bài báo “Word Space” (Không gian từ ngữ) năm 1993, trong đó ông đã chỉ ra cách tính “Word Vector” (vector từ) bằng việc xử lý ma trận trên số lượng cụm từ đồng xuất hiện, dẫn đến khả năng tìm thấy các từ tương tự nhất cho bất kỳ từ ngữ truy vấn nào:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8551f3993548972034bc7656ecf267d8/756c3/93765351de894131bde03736d4fd8e6abde6d1cb.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.35294117647059%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdaAWAf/xAAYEAACAwAAAAAAAAAAAAAAAAAAAQIiMf/aAAgBAQABBQJpliOf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAABBB/9oACAEBAAY/Air/xAAZEAACAwEAAAAAAAAAAAAAAAAAEQExQfH/2gAIAQEAAT8hU2xdCE2Z/9oADAMBAAIAAwAAABBwD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABkQAQACAwAAAAAAAAAAAAAAAAERIQAQQf/aAAgBAQABPxBkCbOnUGuSvc//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"93765351de894131bde03736d4fd8e6abde6d1cb.jpeg\"\n        title=\"93765351de894131bde03736d4fd8e6abde6d1cb.jpeg\"\n        src=\"/static/8551f3993548972034bc7656ecf267d8/7bf67/93765351de894131bde03736d4fd8e6abde6d1cb.jpg\"\n        srcset=\"/static/8551f3993548972034bc7656ecf267d8/651be/93765351de894131bde03736d4fd8e6abde6d1cb.jpg 170w,\n/static/8551f3993548972034bc7656ecf267d8/d30a3/93765351de894131bde03736d4fd8e6abde6d1cb.jpg 340w,\n/static/8551f3993548972034bc7656ecf267d8/7bf67/93765351de894131bde03736d4fd8e6abde6d1cb.jpg 680w,\n/static/8551f3993548972034bc7656ecf267d8/990cb/93765351de894131bde03736d4fd8e6abde6d1cb.jpg 1020w,\n/static/8551f3993548972034bc7656ecf267d8/c44b8/93765351de894131bde03736d4fd8e6abde6d1cb.jpg 1360w,\n/static/8551f3993548972034bc7656ecf267d8/756c3/93765351de894131bde03736d4fd8e6abde6d1cb.jpg 1564w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Sử dụng vector từ để tìm các từ giống nhất với một số từ truy vấn. (</em><em><a href=\"https://wordrepr.danieldk.eu/schuetze-1993.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Vậy thì, mạng nơ-ron ở đâu trong bức tranh này? Có nhiều cách để tính vector từ và cách tiếp cận của bài báo trên chỉ là một trong số đó. Chuyện gì sẽ xảy nếu ta sử dụng các vector từ làm đầu vào cho mạng nơ-ron đã được tối ưu để thực hiện mô hình hóa ngôn ngữ một cách chính xác? Và sau đó, cả vector được liên kết với từ và khả năng tổng thể của mạng nơ-ron thực hiện mô hình hóa ngôn ngữ có thể được tối ưu hóa cùng nhau bằng cách sử dụng lan truyền ngược từ hàm lỗi thích hợp. Đây là nơi chúng ta quay lại với mô hình ngôn ngữ xác suất nơ-ron (Neural Probabilistic Language Model), vì về cơ bản đó là những gì bài báo mô tả.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7a9f1975df2aaf03faa7b898c37ef302/bc229/099743d6428c2394206026466711fd74d02146d7.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.94117647058823%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe9JTQP/xAAWEAEBAQAAAAAAAAAAAAAAAAAQAUH/2gAIAQEAAQUCdh//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAACAgMAAAAAAAAAAAAAAAAAARARMUFR/9oACAEBAAE/IWLO4viCP//aAAwDAQACAAMAAAAQow//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAEAAgIDAQAAAAAAAAAAAAABABFBcRAhMaH/2gAIAQEAAT8Q3dGYHL44Q9sKhBaPCorJ/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"099743d6428c2394206026466711fd74d02146d7.jpeg\"\n        title=\"099743d6428c2394206026466711fd74d02146d7.jpeg\"\n        src=\"/static/7a9f1975df2aaf03faa7b898c37ef302/7bf67/099743d6428c2394206026466711fd74d02146d7.jpg\"\n        srcset=\"/static/7a9f1975df2aaf03faa7b898c37ef302/651be/099743d6428c2394206026466711fd74d02146d7.jpg 170w,\n/static/7a9f1975df2aaf03faa7b898c37ef302/d30a3/099743d6428c2394206026466711fd74d02146d7.jpg 340w,\n/static/7a9f1975df2aaf03faa7b898c37ef302/7bf67/099743d6428c2394206026466711fd74d02146d7.jpg 680w,\n/static/7a9f1975df2aaf03faa7b898c37ef302/990cb/099743d6428c2394206026466711fd74d02146d7.jpg 1020w,\n/static/7a9f1975df2aaf03faa7b898c37ef302/c44b8/099743d6428c2394206026466711fd74d02146d7.jpg 1360w,\n/static/7a9f1975df2aaf03faa7b898c37ef302/bc229/099743d6428c2394206026466711fd74d02146d7.jpg 1554w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Sơ đồ mạng nơ-ron được sử dụng để mô hình hóa ngôn ngữ trong bài báo này (</em><em><a href=\"https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Đây hiện là một công trình được trích dẫn rất nhiều, nhưng cũng như nhiều công trình khác mà chúng ta đã thấy ở trên, phải mất một thời gian để tính hữu ích của vector từ và mạng nơ-ron dùng cho mô hình hóa ngôn ngữ được công nhận. Chúng ta sẽ hiểu vì sao ở phần tiếp theo.</p>\n<h2>Một mùa đông AI mới bắt đầu nhen nhóm</h2>\n<p>Quay lại những năm 90 - vẫn còn một vấn đề. Một vấn đề lớn, mà gần đây đã có sự tiến bộ vượt bậc - lan truyền ngược. Mạng nơ-ron tích chập quan trọng một phần vì kỹ thuật lan truyền ngược không hoạt động tốt trên mạng nơ-ron bình thường có nhiều lớp. Và đó là chìa khóa thực sự dẫn đến học sâu - có nhiều lớp, trong các hệ thống ngày nay có tới 20 lớp trở lên. Nhưng đến cuối những năm 1980, người ta đã biết rằng các mạng nơ-ron sâu được huấn luyện với kỹ thuật lan truyền ngược không hoạt động tốt lắm và đặc biệt là không hoạt động tốt như các mạng có ít lớp hơn. Lý do, theo thuật ngữ cơ bản, là kỹ thuật lan truyền ngược dựa vào việc tìm ra lỗi ở lớp đầu ra và liên tiếp phân chia lỗi cho các lớp trước đó. Khi có nhiều lớp, sự phân chia lỗi dựa trên phép tính này kết thúc với những con số quá lớn (bùng nổ gradient) hoặc quá bé (tiêu biến gradient), và kết quả là mạng nơ-ron không hoạt động tốt lắm. Jurgen Schmidhuber, một chuyên gia khác về học sâu, cách giải thích chính thức hơn được tóm tắt như sau:</p>\n<p>Kỹ thuật lan truyền ngược liên hồi về cơ bản tương đương với một mạng nơ-ron nhiều lớp, vì vậy RNN đặc biệt khó huấn luyện bằng lan truyền ngược. Cả Sepp Hochreiter (được cố vấn bởi Schmidhuber) và Yoshua Bengio đều xuất bản các bài báo về việc không thể học thông tin lâu dài do hạn chế của lan truyền ngược. Việc phân tích vấn đề đã cho thấy một giải pháp - Schmidhuber và Hochreiter đã đưa ra một khái niệm rất quan trọng vào năm 1997, cơ bản giải quyết vấn đề về cách huấn luyện RNN, giống như CNN đã làm với mạng truyền xuôi - bộ nhớ ngắn hạn dài (<a href=\"http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Long Short Term Memory</a> - LSTM). Nói một cách dễ hiểu, cũng như với CNN, bước đột phá LSTM cuối cùng chỉ là một thay đổi nhỏ đối với mô hình mạng nơ-ron bình thường:</p>\n<p>Tuy nhiên, điều này không giúp giải quyết được vấn đề lớn hơn về nhận thức rằng mạng nơ-ron rất khó và hoạt động không tốt. Chúng vẫn còn những bất tiện - máy tính không đủ nhanh, thuật toán không đủ thông minh và mọi người không hài lòng. Vì vậy, vào khoảng giữa những năm 90, một ”mùa đông AI” mới của mạng nơ-ron bắt đầu xuất hiện - cộng đồng một lần nữa mất niềm tin vào chúng. Một phương pháp mới được gọi là Support Vector Machines (SVM), hiểu đơn giản là một cách tối ưu về mặt toán học để huấn luyện một mô hình tương đương với mạng nơ-ron hai lớp đã được phát triển và bắt đầu tỏ ra vượt trội so với những khó khăn mà mạng nơ-ron đem lại. Trên thực tế, bài báo “Comparison of Learning Algorithms For Handwritten Digit Recognition” (So sánh các thuật toán học để nhận dạng chữ số viết tay) năm 1995 của LeCun và cộng sự nhận thấy rằng cách tiếp cận mới này hoạt động tốt hơn hoặc bằng với các phương pháp khác trừ những thiết kế mạng nơ-ron tốt nhất:</p>\n<p>Các phương pháp mới khác, đặc biệt là Random Forest, cũng tỏ ra rất hiệu quả và với lý thuyết toán đằng sau chúng. Vì vậy, mặc dù thực tế là CNN vẫn giữ được thế dẫn đầu về hiệu năng, sự hứng thú dành cho mạng nơ-ron đã tiêu tan và cộng đồng học máy lại lần nữa chối từ nó. ”Mùa đông AI” đã trở lại. Nhưng đừng lo, tiếp theo chúng ta sẽ xem cách một nhóm nhỏ các nhà nghiên cứu đã kiên trì theo đuổi và cuối cùng tạo ra học sâu (Deep Learning) như ngày nay.</p>\n<hr>\n<p><em>Dịch từ bài viết</em> <strong><em><a href=\"https://www.skynettoday.com/overviews/neural-net-history\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">A Brief History of Neural Nets and Deep Learning</a></em></strong> <em>, Andrey Kurenkov.</em></p>\n<hr>","frontmatter":{"title":"Lược sử mạng nơ-ron và học sâu (Phần 2)","description":"Thời kỳ nở hoa của mạng nơ-ron (những năm 1980-2000).","image":{"childImageSharp":{"resize":{"src":"/static/9859fdc9bf7bcd8f3829c263beb813ee/c7a62/2a11b49113b4847fa2304e22b339633af8dc42bd.jpg","height":630,"width":1200}}}}}},"pageContext":{"slug":"/luoc-su-mang-no-ron-va-hoc-sau-phan-2/","previousPost":{"title":"Smartphone đang khiến bạn ngu đi, sống khép kín và kém lành mạnh. Nhưng tại sao bạn không thể đặt nó xuống?","slug":"/smartphone-dang-khien-ban-ngu-di-song-khep-kin-va-kem-lanh-manh-nhung-tai-sao-ban-khong-the-dat-no-xuong/"},"nextPost":{"title":"Lược sử mạng nơ-ron và học sâu (Phần 1)","slug":"/luoc-su-mang-no-ron-va-hoc-sau-phan-1/"}}},"staticQueryHashes":["3868140423","4085307986"]}