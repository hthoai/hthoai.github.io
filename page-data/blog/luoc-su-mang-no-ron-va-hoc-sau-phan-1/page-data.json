{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/luoc-su-mang-no-ron-va-hoc-sau-phan-1/","result":{"data":{"markdownRemark":{"html":"<p><em>Câu chuyện về sự phát triển của mạng nơ-ron từ những ngày đầu của trí tuệ nhân tạo đến nay.</em></p>\n<h1>Phần mở đầu: Cơn sóng thần học sâu</h1>\n<p>Nghe có vẻ hơi cường điệu, nhưng các phương pháp đã được thiết lập trước đây trong toàn bộ lĩnh vực nghiên cứu đang nhanh chóng bị thay thế bởi một khám phá mới, như thể bị tấn công bởi một cơn sóng thần. Tuy nhiên, ngôn ngữ thảm khốc này phù hợp để mô tả sự nổi lên như vũ bão của học sâu trong vài năm trở lại đây - thể hiện qua những cải tiến mạnh mẽ đối với những phương pháp vốn thống trị những vấn đề khó nhằn của AI, và sự đầu tư từ những gã khổng lồ trong ngành như Google, kéo theo sự tăng trưởng theo cấp số nhân các ấn phẩm nghiên cứu (và những nghiên cứu sinh học máy). Đã từng tham gia một số lớp về Học máy, cũng như dùng nó cho nghiên cứu ở đại học, tôi không khỏi tự hỏi liệu học sâu có gì thú vị hay chỉ là một phiên bản lớn hơn của mạng nơ-rơn nhân tạo đã được phát triển vào những năm cuối thập niên 80. Hãy để tôi kể bạn nghe, câu trả lời là một câu chuyện - câu chuyện không chỉ về mạng nơ-ron hay một chuỗi các đột phá trong nghiên cứu khiến học sâu thú vị hơn “mạng nơ-ron lớn” (điều mà tôi sẽ cố gắng giải thích sao cho người ngoại đạo cũng có thể hiểu), nhưng trên hết là cách một số nhà nghiên cứu kiên cường đã vượt qua “mùa đông đen tối” của AI để mang mạng nơ-ron trở lại và đạt được giấc mơ học sâu.</p>\n<h1>Phần 1: Thời kỳ đầu (những năm 1950-1980)</h1>\n<p>Ở phần này, tôi sẽ đề cập đến sự ra đời của mạng nơ-ron vào năm 1958, thời kỳ vỡ mộng (the AI Winter) vào những năm 70, và sự phổ biến trở lại nhờ thuật toán lan truyền ngược vào năm 1986.</p>\n<h2>Thuật toán học máy xưa cũ</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a6911e20671db84be938dad29088c2e/b8167/263ed0c010af043a17371386ca52ede5dfba0449.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.88235294117646%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd9SyiA//8QAFhABAQEAAAAAAAAAAAAAAAAAABEg/9oACAEBAAEFAsVX/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAEQETFBUf/aAAgBAQABPyFl8UZg66P/2gAMAwEAAgADAAAAEBjP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHhABAAICAQUAAAAAAAAAAAAAAQAhETFxQVHR4fD/2gAIAQEAAT8QQIrUWm7moCdZkSpq7eZn2+rzF9k//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"263ed0c010af043a17371386ca52ede5dfba0449.jpeg\"\n        title=\"263ed0c010af043a17371386ca52ede5dfba0449.jpeg\"\n        src=\"/static/4a6911e20671db84be938dad29088c2e/7bf67/263ed0c010af043a17371386ca52ede5dfba0449.jpg\"\n        srcset=\"/static/4a6911e20671db84be938dad29088c2e/651be/263ed0c010af043a17371386ca52ede5dfba0449.jpg 170w,\n/static/4a6911e20671db84be938dad29088c2e/d30a3/263ed0c010af043a17371386ca52ede5dfba0449.jpg 340w,\n/static/4a6911e20671db84be938dad29088c2e/7bf67/263ed0c010af043a17371386ca52ede5dfba0449.jpg 680w,\n/static/4a6911e20671db84be938dad29088c2e/990cb/263ed0c010af043a17371386ca52ede5dfba0449.jpg 1020w,\n/static/4a6911e20671db84be938dad29088c2e/c44b8/263ed0c010af043a17371386ca52ede5dfba0449.jpg 1360w,\n/static/4a6911e20671db84be938dad29088c2e/b8167/263ed0c010af043a17371386ca52ede5dfba0449.jpg 1458w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Hồi quy tuyến tính (</em><em><a href=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Hãy bắt đầu với phần sơ khởi của học máy. Cho một vài điểm trên đồ thị, vẽ một đường thẳng sao cho khoảng cách giữa mỗi điểm đến nó là ngắn nhất có thể. Những gì bạn làm là tổng quát hóa từ mẫu các cặp giá trị đầu vào (⁍) - giá trị đầu ra (⁍) thành một hàm tổng quát có thể ánh xạ bất kỳ giá trị đầu vào nào thành giá trị đầu ra. Đây là một kỹ thuật 200 năm tuổi tuyệt vời để ngoại suy một hàm tổng quát từ tập hợp các cặp giá trị đầu vào - đầu ra. Và đây là lý do tại sao một kỹ thuật như thế lại tuyệt vời: có một lượng không đếm xuể các hàm rất khó để phát triển một cách trực tiếp, nhưng lại dễ dàng hơn khi thu thập các mẫu đầu vào - đầu ra trong thực tế, ví dụ như hàm ánh xạ đầu vào là âm thanh của một từ sang đầu ra là ký tự biểu diễn của từ đó.</p>\n<p>Hồi quy tuyến tính là một kỹ thuật hơi yếu kém trong việc giải quyết bài toán nhận dạng giọng nói, nhưng về cơ bản những gì nó làm là điển hình của học có giám sát: học một hàm từ tập các mẫu huấn luyện, trong đó mỗi mẫu là cặp giá trị đầu vào - đầu ra có được từ hàm đấy (giả sử hàm này tồn tại). Đặc biệt, các phương pháp học máy có thể cho kết quả là một hàm có khả năng tổng quát hóa tốt cho các giá trị đầu vào không có trong tập huấn luyện, từ đó ta có thể áp dụng nó cho các đầu vào mà chúng ta chưa biết đầu ra. Ví dụ, công nghệ nhận dạng giọng nói của Google được áp dụng học máy với tập huấn luyện khổng lồ, nhưng gần như nó không bao trùng tất cả những gì bạn sẽ nói với chiếc điện thoại của mình.</p>\n<p>Nguyên tắc tổng quát hóa (generalization) này quan trọng đến mức hầu như sẽ luôn có một tập dữ liệu kiểm tra (thêm một lượng mẫu đầu vào - đầu ra và chúng không nằm trong tập huấn luyện). Tập dữ liệu riêng biệt này được sử dụng để đánh giá hiệu quả của kỹ thuật học máy bằng cách kiểm tra xem có bao nhiêu mẫu mà phương pháp đang sử dụng tính đúng đầu ra với đầu vào cho trước. Kẻ thù của tổng quát hóa là quá khớp (overfit) - học một hàm cho kết quả rất tốt trên tập huấn luyện nhưng lại rất kém trên tập kiểm tra. Vì các nhà nghiên cứu học máy cần các phương tiện để so sánh hiệu quả những phương pháp của họ nên theo thời gian đã xuất hiện các bộ dữ liệu tiêu chuẩn với tập huấn luyện và tập kiểm tra dùng để đánh giá các thuật toán học máy.</p>\n<p>Thế là cũng đủ cho các định nghĩa rồi. Mấu chốt ở đây là - bài tập vẽ đường thẳng ở trên là một ví dụ rất đơn giản của học có giám sát: các điểm cho trước là tập huấn luyện (⁍ - đầu vào, ⁍ - đầu ra), đường thẳng là hàm xấp xỉ, và ta có thể dùng đường thẳng này để tìm giá trị ⁍ cho những ⁍ không sẵn có từ đầu. Đừng lo lắng, phần còn lại của lược sử này sẽ không quá khô khan như những điều vừa kể trên. Bắt đầu nào.</p>\n<h2>Những hứa hẹn viển vông</h2>\n<p>Tại sao tất cả trong phần mở đầu lại là hồi quy tuyến tính, trong khi chủ đề ở đây rõ ràng là mạng nơ-ron? Chuyện là, hồi quy tuyến tính có một số điểm tương đồng với ý tưởng đầu tiên được hình thành một cách cụ thể như một phương pháp giúp máy có khả năng học: <a href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&#x26;id=1959-09865-001\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Frank Rosenblatt’s Perceptron</a>.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c7d6791e84c1bacf8afb35c8d27d89eb/d33ce/aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 30%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3aAH/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAERIVH/2gAIAQEAAT8hStmab0VP/9oADAMBAAIAAwAAABBzz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABkQAQADAQEAAAAAAAAAAAAAAAEAESExYf/aAAgBAQABPxANaunuxKmM87Gi2p//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpeg\"\n        title=\"aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpeg\"\n        src=\"/static/c7d6791e84c1bacf8afb35c8d27d89eb/7bf67/aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpg\"\n        srcset=\"/static/c7d6791e84c1bacf8afb35c8d27d89eb/651be/aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpg 170w,\n/static/c7d6791e84c1bacf8afb35c8d27d89eb/d30a3/aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpg 340w,\n/static/c7d6791e84c1bacf8afb35c8d27d89eb/7bf67/aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpg 680w,\n/static/c7d6791e84c1bacf8afb35c8d27d89eb/d33ce/aba0d783bb0be62159c6ef4d8bb7a1cc1a97e379.jpg 715w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Sơ đồ thể hiện cách hoạt động của Perceptron. (</em><em><a href=\"http://cse-wiki.unl.edu/wiki/images/0/0f/Perceptron.jpg\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Rosenblatt - một nhà tâm lý học - quan niệm Perceptron như một mô hình toán học đơn giản về cách các nơ-ron (tế bào thần kinh) trong não của chúng ta hoạt động: nó nhận tập các đầu vào nhị phân (các nơ-ron lân cận), nhân mỗi đầu vào với một trọng số có giá trị liên tục (thể hiện độ mạnh của các khớp thần kinh tới mỗi nơ-ron lân cận), và xuất ra 1 nếu tổng của các đầu vào có trọng số này lớn hơn một ngưỡng nhất định, ngược lại thì xuất ra 0 (tương tự như cách các nơ-ron kích hoạt hoặc không kích hoạt). Hầu hết các đầu vào cho Perceptron là một số mẫu dữ liệu hoặc là đầu ra của một Perceptron khác, nhưng được thêm vào một hệ số điều chỉnh (bias) có giá trị là 1, để đảm bảo có thể tính toán nhiều hàm hơn với cùng một đầu vào bằng khả năng bù trừ cho giá trị tổng. Mô hình nơ-ron này được xây dựng dựa trên công trình của Warren McCulloch và Walter Pitts - Mcculoch-Pitts, những người đã chỉ ra rằng một mô hình nơ-ron tính tổng các đầu vào nhị phân và cho ra 1 nếu tổng lớn hơn một ngưỡng nhất định, ngược lại kết quả là 0, có thể mô hình hóa các hàm cơ bản như OR/AND/NOT. Kết quả này trong thời kỳ đầu của AI là một thành công lớn - khi suy nghĩ chủ yếu lúc đó là làm sao cho máy tính có thể thực hiện suy luận logic, từ đó giải quyết các bài toán AI.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 659px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/eb822438297f63149de8a3c621f09588/fff7d/3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.05882352941176%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe7liVD/xAAWEAEBAQAAAAAAAAAAAAAAAAARACD/2gAIAQEAAQUCJnH/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAACAwEAAAAAAAAAAAAAAAAAARARcTH/2gAIAQEAAT8heiyRgfClH//aAAwDAQACAAMAAAAQw8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEBAAMBAAAAAAAAAAAAAAABEQAhMRD/2gAIAQEAAT8QCEFu9wCopYXuXNNwjCZo4ef/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpeg\"\n        title=\"3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpeg\"\n        src=\"/static/eb822438297f63149de8a3c621f09588/fff7d/3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpg\"\n        srcset=\"/static/eb822438297f63149de8a3c621f09588/651be/3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpg 170w,\n/static/eb822438297f63149de8a3c621f09588/d30a3/3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpg 340w,\n/static/eb822438297f63149de8a3c621f09588/fff7d/3b8846b6fe451ed95199eb6b2570aafe74d7bda1.jpg 659w\"\n        sizes=\"(max-width: 659px) 100vw, 659px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một sơ đồ khác, thể hiện nguồn cảm hứng từ sinh học. Hàm kích hoạt - hiện nay được gọi là hàm phi tuyến - áp dụng cho tổng các đầu vào có trọng số để tạo ra đầu ra của nơ-ron nhân tạo - trong mô hình Perceptron thực chất là một phép toán ngưỡng. (</em><em><a href=\"http://cs231n.github.io/neural-networks-1/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Tuy nhiên, mô hình Mcculoch-Pitts thiếu một cơ chế cho việc học, điều cốt yếu để có thể sử dụng được cho AI. Đây là điểm mà Perceptron tỏ ra thắng thế - Rosenblatt đã nghĩ ra một cách giúp các nơ-ron nhân tạo như trên học được, lấy cảm hứng từ công trình nền tảng của Donald Hebb. Hebb đưa ra một ý tưởng bất ngờ và có ảnh hưởng lớn rằng kiến thức và việc học xảy ra trong não chủ yếu dựa trên sự hình thành và thay đổi của các khớp thần kinh giữa các nơ-ron - gọi ngắn gọn là quy tắc Hebb. Quy tắc Hebb nói rằng nếu hai tế bào thần kinh hoạt động gần như cùng lúc thì các kết nối của chúng được tăng cường. Cụ thể:</p>\n<p>Perceptron không thực hiện chính xác ý tưởng này, mà có trọng số trên các đầu vào cho phép thể hiện một sơ đồ học rất đơn giản và trực quan: với tập các mẫu đầu vào - đầu ra cho sẵn, Perceptron sẽ học một hàm từ đó, với mỗi mẫu, tăng trọng số nếu Perceptron cho ra kết quả quá thấp so với đầu ra của mẫu đó, và ngược lại, giảm trọng số nếu kết quả quá cao. Cụ thể, thuật toán được trình bày như sau:</p>\n<p>Quy trình này đơn giản, và tạo ra một kết quả đơn giản: một hàm tuyến tính đầu vào (tổng có trọng số), giống với hồi quy tuyến tính, bị “bẻ” bởi một hàm kích hoạt phi tuyến (ngưỡng của tổng), tạo ra một tập hữu hạn các giá trị đầu ra (như trong trường hợp các hàm logic là 2 - Đúng/1 và Sai/0). Do đó, vấn đề không còn là tạo ra kết quả với các giá trị liên tục - hồi quy, mà là phân loại các đầu vào vào đúng nhãn của chúng - phân lớp.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 285px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/46c099e74bd86f09430aef0945ccfe94/6c669/25f56cf5c886f59388eb42e84e7fd2ce0094f076.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 122.94117647058822%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAZABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAv/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHI+aHCgonpULNh/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAECEgMxMkH/2gAIAQEAAQUCtK1mKTM/QjP0yOvRH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8BH//EABsQAAEEAwAAAAAAAAAAAAAAAAABAhDhIFFx/9oACAEBAAY/And2WXgkun//xAAeEAACAQQDAQAAAAAAAAAAAAAAASERMUFhUZGhsf/aAAgBAQABPyFyVb0pD29ynz3IJHJseBvYopc2fR+zHdGZ/9oADAMBAAIAAwAAABDc/sP/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAiEAEAAgAGAQUAAAAAAAAAAAABABEhMUFRYXGRobHB0fD/2gAIAQEAAT8QNaYCIYXwzQeP3wTseOsXgUro7itWKoyeoDpz4huONHullKZMrbltsn7G89ZMnb4J/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"25f56cf5c886f59388eb42e84e7fd2ce0094f076.jpeg\"\n        title=\"25f56cf5c886f59388eb42e84e7fd2ce0094f076.jpeg\"\n        src=\"/static/46c099e74bd86f09430aef0945ccfe94/6c669/25f56cf5c886f59388eb42e84e7fd2ce0094f076.jpg\"\n        srcset=\"/static/46c099e74bd86f09430aef0945ccfe94/651be/25f56cf5c886f59388eb42e84e7fd2ce0094f076.jpg 170w,\n/static/46c099e74bd86f09430aef0945ccfe94/6c669/25f56cf5c886f59388eb42e84e7fd2ce0094f076.jpg 285w\"\n        sizes=\"(max-width: 285px) 100vw, 285px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Mark I Perceptron tại phòng thí nghiệm hàng không Cornell', triển khai phần cứng đầu tiên của Perceptron. (Nguồn: Wikipedia / Cornell Library)</em></p>\n<p>Rosenblatt đã triển khai ý tưởng về Perceptron trên phần cứng tùy chỉnh (đấy là trước khi các ngôn ngữ lập trình thông dụng cho học máy xuất hiện) và cho thấy nó có thể học để phân loại những hình dạng đơn giản một cách chính xác với đầu vào dạng pixel 20x20. Và như vậy, học máy đã được sinh ra - một chiếc máy tính đã có thể xấp xỉ một hàm từ các cặp đầu vào - đầu ra cho trước. Trong trường hợp này, máy chỉ mới học được một hàm đơn giản, nhưng không khó để hình dung ra những ứng dụng hữu ích như chuyển đổi mớ chữ viết tay hỗn độn của con người sang những ký tự mà máy có thể đọc được.</p>\n<p>Nhưng chờ đã, chúng ta chỉ mới thấy cách một Perceptron học để cho ra 0 hoặc 1 - làm sao nó có thể mở rộng để hoạt động với các bài toán phân loại nhiều nhãn như là chữ viết tay? Điều này là không thể với một Perceptron, vì nó chỉ có một đầu ra, nhưng những hàm với nhiều đầu ra có thể được học từ nhiều Perceptrons trong một lớp, sao cho tất cả Perceptrons này nhận cùng một đầu vào và mỗi cái chịu trách nhiệm cho một đầu ra của hàm. Mạng nơ-ron (hay cụ thể là mạng nơ-ron nhân tạo - ANNs) thực chất không gì hơn là các lớp Perceptrons, hoặc các nơ-ron, hoặc các unit (node) như cách chúng được gọi ngày nay, mà ở giai đoạn này chỉ có một lớp - lớp đầu ra. Một ví dụ điển hình của việc sử dụng mạng nơ-ron là phân loại ảnh chữ số viết tay. Đầu vào là các giá trị pixel của ảnh, và sẽ có 10 nơ-ron đầu ra, mỗi nơ-ron ứng với một trong 10 giá trị chữ số có thể có. Trong trường hợp này, chỉ một trong 10 nơ-ron xuất ra 1, nơ-ron với tổng có trọng số cao nhất được coi là đầu ra chính xác - 1, phần còn lại có đầu ra là 0.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 260px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a748e906ef2a88eedfd6280c9b2c62ce/ab8e0/d13b9b64336ff3565d4b2d4eb7ea7a99682eb9cc.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100.58823529411765%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAIBAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAH24uDoDNCgf//EABkQAAMBAQEAAAAAAAAAAAAAAAABMQIhQf/aAAgBAQABBQL3UHXxGDcU/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAGRAAAgMBAAAAAAAAAAAAAAAAADEQESBB/9oACAEBAAY/Ao6MdDx//8QAGxAAAwEAAwEAAAAAAAAAAAAAAAERIRAxwWH/2gAIAQEAAT8hbjdj3VnwyIajXh7BFxFm07w9B//aAAwDAQACAAMAAAAQ4888/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPxAf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPxAf/8QAIBAAAgICAAcAAAAAAAAAAAAAAREAITFhQVFxgZGh8P/aAAgBAQABPxBwB3Ba7QBwd1gCzqDgMRzhMBgOPmYKri8DXiAMHAur3GsdwkfWegJ//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"d13b9b64336ff3565d4b2d4eb7ea7a99682eb9cc.jpeg\"\n        title=\"d13b9b64336ff3565d4b2d4eb7ea7a99682eb9cc.jpeg\"\n        src=\"/static/a748e906ef2a88eedfd6280c9b2c62ce/ab8e0/d13b9b64336ff3565d4b2d4eb7ea7a99682eb9cc.jpg\"\n        srcset=\"/static/a748e906ef2a88eedfd6280c9b2c62ce/651be/d13b9b64336ff3565d4b2d4eb7ea7a99682eb9cc.jpg 170w,\n/static/a748e906ef2a88eedfd6280c9b2c62ce/ab8e0/d13b9b64336ff3565d4b2d4eb7ea7a99682eb9cc.jpg 260w\"\n        sizes=\"(max-width: 260px) 100vw, 260px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Một mạng nơ-ron với nhiều đầu ra.</em></p>\n<p>Ta cũng có thể xem mạng nơ-ron với các nơ-ron nhân tạo khác biệt với Perceptrons. Ví dụ, hàm kích hoạt ngưỡng không còn quá cần thiết, Bernard Widrow và Tedd Hoff đã sớm khám phá ra phương pháp chỉ xuất đầu vào có trọng số (thay vì dựa vào ngưỡng để xuất 0 hoặc 1) vào năm 1960 qua bài báo <a href=\"http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">“An adaptive</a> <em><a href=\"http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ADALINE</a></em>  <a href=\"http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">neuron using chemical</a> <em><a href=\"http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">memistors</a></em> <a href=\"http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">”</a>, và chỉ ra cách những ‘Adaptive Linear Neurons’ này có thể được kết hợp vào các mạch điện với ‘memistors’ - điện trở có bộ nhớ. Họ cũng chỉ ra rằng việc không có hàm kích hoạt ngưỡng sẽ tốt cho việc tính toán, bởi vì cơ chế học của nơ-ron có thể dựa trên việc giảm thiểu lỗi thông qua những phép tính tốt. Xét việc hàm nơ-ron không bị “bẻ” về đoạn 0-1 thì một phép đo sự thay đổi của độ lỗi mỗi khi trọng số thay đổi (đạo hàm) có thể được sử dụng để giúp độ lỗi của mạng giảm nhanh hơn và tìm ra giá trị tối ưu của trọng số. Và như chúng ta đã thấy, việc tìm ra bộ trọng số phù hợp dựa vào đạo hàm trên độ lỗi của tập huấn luyện ứng với mỗi trọng số chính là cách mạng nơ-ron được huấn luyện ngày nay.</p>\n<p>Khi suy xét kỹ hơn về ADALINE (không sử dụng hàm kích hoạt ngưỡng như đã trình bày phía trên), ta sẽ thấy rõ một điều: việc tìm tập các trọng số phù hợp cho một lượng mẫu đầu vào nhất định thực chất chỉ là một dạng của hồi quy tuyến tính. Và ta lại quay về câu chuyện cũ, như với hồi quy tuyến tính, điều này vẫn chưa đủ để giải quyết các vấn đề phức tạp hơn của AI như nhận dạng giọng nói hoặc thị giác máy tính. Điều McCullough, Pitts, và Rosenblatt thấy hứng thú là một ý tưởng rộng hơn về chủ nghĩa kết nối: những mạng lưới nơ-ron với kiểu tính toán đơn giản như trên có thể trở nên mạnh hơn và giải quyết được các vấn đề nan giải của AI. Rosenblatt đã nói rất nhiều, như trong trích dẫn có phần khôi hài này của New York Times vào thời điểm đó:</p>\n<p>Hoặc bạn có thể xem đoạn video dưới đây (xuất hiện vào thời điểm đó):</p>\n<p><div class=\"gatsby-resp-iframe-wrapper\" style=\"padding-bottom: 56.42857142857143%; position: relative; height: 0; overflow: hidden; \" > <div class=\"embedVideo-container\"> <iframe title src=\"https://www.youtube.com/embed/aygSMgK3BEM?rel=0\" class=\"embedVideo-iframe\" style=\"border:0; position: absolute; top: 0; left: 0; width: 100%; height: 100%; \" allowfullscreen></iframe> </div> </div></p>\n<p><em>Những lời hứa hẹn trong video này cho đến nay vẫn chưa thấy đâu.</em></p>\n<p>Cách nói như trên có thể khiến những nhà nghiên cứu AI khác cảm thấy khó chịu, khi nhiều người trong số họ tập trung vào các phương pháp tiếp cận dựa trên quy tắc cụ thể, tuân theo logic toán học. Marvin Minsky, người thành lập phòng thí nghiệm AI tại MIT, và Seymour Papert, giám đốc phòng thí nghiệm vào thời điểm đó, là hai trong số những nhà nghiên cứu cho rằng Perceptrons đã bị thổi phồng quá mức. Vào năm 1969, họ đã khéo léo thể hiện sự hoài nghi của mình bằng những phân tích chặt chẽ về những hạn chế của Perceptrons trong một cuốn sách có tên Perceptrons. Thú vị ở chỗ, Minksy có lẽ là nhà nghiên cứu đầu tiên triển khai phần cứng cho mạng nơ-ron với máy SNARC (Stochastic Neural Analog Reinforcement Calculator), trước cả Rosenblatt nhiều năm. Nhưng việc thiếu các dấu vết về những hoạt động trên hệ thống này, cùng với những phân tích quan trọng ở cuốn Perceptrons đã khiến ông đi đến kết luận rằng tiếp cận AI theo hướng này đã đi vào ngõ cụt. Yếu tố được thảo luận rộng rãi ở phân tích này là việc làm sáng tỏ những giới hạn của Perceptron, ví dụ, chúng không thể học được một hàm boolean đơn giản như XOR vì XOR không khả tách tuyến tính. Chuyện này đến nay vẫn còn khá mơ hồ, nhưng nhiều người tin rằng ấn phẩm trên là khởi điểm cho “mùa đông AI thứ nhất” - một giai đoạn mà sự thổi phồng quá lớn về sức mạnh của AI đã nổ tung, kéo theo tình trạng đóng băng tài trợ cho các dự án nghiên cứu, xuất phẩm.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 510px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7cd903a04727f222ac8e2f65c6f146e7/18815/9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.05882352941176%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe9JQwP/xAAaEAABBQEAAAAAAAAAAAAAAAAAAQIQETJC/9oACAEBAAEFAos6TTT/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAEBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQEABj8CxVf/xAAbEAACAgMBAAAAAAAAAAAAAAAAASFBMWGxcf/aAAgBAQABPyFOdisSVIs/DhOh/9oADAMBAAIAAwAAABBjD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB4QAQABBAIDAAAAAAAAAAAAAAEAESExYRBBUXGh/9oACAEBAAE/EKiL6RGCr2eGUNzsNZ8fiMPbP//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpeg\"\n        title=\"9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpeg\"\n        src=\"/static/7cd903a04727f222ac8e2f65c6f146e7/18815/9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpg\"\n        srcset=\"/static/7cd903a04727f222ac8e2f65c6f146e7/651be/9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpg 170w,\n/static/7cd903a04727f222ac8e2f65c6f146e7/d30a3/9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpg 340w,\n/static/7cd903a04727f222ac8e2f65c6f146e7/18815/9834a734c5ed5c4ef4a1c85fd4bc9b979e017311.jpg 510w\"\n        sizes=\"(max-width: 510px) 100vw, 510px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Minh họa những hạn chế của Perceptron. Tìm một hàm tuyến tính từ các cặp đầu vào X, Y để phân chia chính xác hai miền +/- bằng một đường thẳng. Rõ ràng, với trường hợp thứ ba, không tồn tại một đường thẳng nào như thế.</em></p>\n<h2>Khi mùa đông đi qua</h2>\n<p>Mọi chuyện có vẻ không khả quan khi dùng mạng nơ-ron để giải quyết. Nhưng tại sao? Suy cho cùng, ý tưởng ở đây là kết hợp nhiều nơ-ron với những phép tính đơn giản để giải quyết những vấn đề phức tạp, thay vì chỉ một nơ-ron đơn lẻ. Nói cách khác, thay vì chỉ có một lớp đầu ra, đầu vào sẽ được kết nối với những nơ-ron bất kỳ ở các lớp khác - gọi là lớp ẩn, trước khi đi đến lớp đầu ra của mạng, đầu ra của lớp này sẽ là đầu vào của lớp kế tiếp. Ta chỉ thấy được kết quả xuất ra của lớp đầu ra - nó là kết quả của bài toán, nhưng tất cả các phép tính trung gian được thực hiện ở các lớp ẩn có thể đã giải quyết nhiều vấn đề phức tạp hơn (điều mà Perceptron đã không làm được khi không có các lớp ẩn này).</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1246afecd55deda70c72f3cf24358360/39240/30a6f9c126743e875c33e0852817c96693a82c49.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.8235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIBAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB2JZasGI//8QAGRAAAgMBAAAAAAAAAAAAAAAAABEBAhAh/9oACAEBAAEFAukNstv/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAADAAMBAAAAAAAAAAAAAAAAAREQIUFR/9oACAEBAAE/IW4flGhRN00ShXVvmP/aAAwDAQACAAMAAAAQoM//xAAWEQADAAAAAAAAAAAAAAAAAAABEDH/2gAIAQMBAT8QNX//xAAWEQEBAQAAAAAAAAAAAAAAAAABECH/2gAIAQIBAT8QDJ//xAAcEAACAgIDAAAAAAAAAAAAAAABEQAxIUEQUWH/2gAIAQEAAT8QOASeyEZ1BRKr8cAIEzlbhs1N8f/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"30a6f9c126743e875c33e0852817c96693a82c49.jpeg\"\n        title=\"30a6f9c126743e875c33e0852817c96693a82c49.jpeg\"\n        src=\"/static/1246afecd55deda70c72f3cf24358360/7bf67/30a6f9c126743e875c33e0852817c96693a82c49.jpg\"\n        srcset=\"/static/1246afecd55deda70c72f3cf24358360/651be/30a6f9c126743e875c33e0852817c96693a82c49.jpg 170w,\n/static/1246afecd55deda70c72f3cf24358360/d30a3/30a6f9c126743e875c33e0852817c96693a82c49.jpg 340w,\n/static/1246afecd55deda70c72f3cf24358360/7bf67/30a6f9c126743e875c33e0852817c96693a82c49.jpg 680w,\n/static/1246afecd55deda70c72f3cf24358360/39240/30a6f9c126743e875c33e0852817c96693a82c49.jpg 791w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Mạng nơ-ron với hai lớp ẩn (</em><em><a href=\"http://cs231n.github.io/neural-networks-1/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Về cơ bản, điều khiến các lớp ẩn thể hiện tốt là chúng có thể tìm được các đặc trưng bên trong dữ liệu và cho phép những lớp kế tiếp tính toán trên những đặc trưng này thay vì những dữ liệu quá lớn và chứa nhiều nhiễu ban đầu. Ví dụ với một nhiệm vụ khá phổ biến của mạng nơ-ron là tìm mặt người trong ảnh, lớp ẩn đầu tiên có thể nhận đầu vào là những giá trị pixel đơn thuần và tìm ra các đặc trưng về đường thẳng, đường tròn, bầu dục... trong ảnh. Lớp kế tiếp sẽ nhận đầu vào là những đặc trưng hình học này để tìm ra vị trí của mặt người trong ảnh - đến đây chuyện đã trở nên dễ dàng hơn so với việc dựa vào những giá trị pixel. Chúng ta cơ bản có thể hiểu được điều này. Trên thực tế, cho đến gần đây thì các kỹ thuật học máy thường không áp dụng trực tiếp lên dữ liệu thô như ảnh hay audio. Thay vào đó, người ta triển khai nó trên dữ liệu đã được trích xuất đặc trưng, giúp cho quá trình học diễn ra dễ dàng hơn khi mà những đặc trưng hữu ích như góc cạnh, hình dạng đã được trích xuất sẵn.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fc494947c315813574cddc961b910f09/c800b/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.88235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAQC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAQAC/9oADAMBAAIQAxAAAAG60HA1f//EABgQAAIDAAAAAAAAAAAAAAAAAAABISMy/9oACAEBAAEFAnWLMM//xAAXEQADAQAAAAAAAAAAAAAAAAAAAREh/9oACAEDAQE/AZgkf//EABcRAAMBAAAAAAAAAAAAAAAAAAEQETL/2gAIAQIBAT8BOov/xAAZEAADAQEBAAAAAAAAAAAAAAAAARECMmH/2gAIAQEABj8C1W9Qvh0z/8QAFxAAAwEAAAAAAAAAAAAAAAAAAAERIf/aAAgBAQABPyGqYzKPV0E0WB//2gAMAwEAAgADAAAAEP8A3//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAgBAwEBPxDIWBNv/8QAFhEBAQEAAAAAAAAAAAAAAAAAARBx/9oACAECAQE/EEmAz//EABkQAQEBAQEBAAAAAAAAAAAAAAERIQBR8P/aAAgBAQABPxAhLFBEqer1EBa10zppFk+O/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpeg\"\n        title=\"64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpeg\"\n        src=\"/static/fc494947c315813574cddc961b910f09/7bf67/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg\"\n        srcset=\"/static/fc494947c315813574cddc961b910f09/651be/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg 170w,\n/static/fc494947c315813574cddc961b910f09/d30a3/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg 340w,\n/static/fc494947c315813574cddc961b910f09/7bf67/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg 680w,\n/static/fc494947c315813574cddc961b910f09/990cb/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg 1020w,\n/static/fc494947c315813574cddc961b910f09/c800b/64e5b6f7d833d8009f69f6cb9a7b00c070686853.jpg 1265w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Minh họa việc trích xuất đặc trưng thủ công (</em><em><a href=\"http://lear.inrialpes.fr/people/vandeweijer/color_descriptors.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Vì vậy, điều quan trọng cần lưu ý là phân tích của Minsky và Papert không chỉ cho thấy sự bất khả thi khi tính XOR với một Perceptron đơn lẻ, mà còn chỉ ra cụ thể rằng nó phải được thực hiện với nhiều Perceptrons - cái mà ngày nay chúng ta gọi là mạng nơ-ron đa lớp - và thuật toán của Rosenblatt thì không hoạt động với nhiều lớp. Đó mới là vấn đề thật sự: quy tắc học đơn giản đã được nêu ra trước đây cho Perceptron không hoạt động với nhiều lớp. Để hiểu vì sao, hãy xem lại cách một Perceptron đơn lẻ học để tính được một hàm nào đó:</p>\n<p>Dễ thấy lý do thuật toán trên không hoạt động với mô hình nhiều lớp: ta chỉ có thể xác định đầu ra chính xác cho lớp cuối cùng, vậy làm sao để điều chỉnh trọng số của những Perceptrons ở những lớp trước đó? Câu trả lời, dù mất một thời gian để tìm ra, lại lần nữa được chứng minh là dựa trên một kiến thức giải tích lâu đời: quy tắc chuỗi (chain rule). Vấn đề cần làm rõ ở đây là nếu mạng nơ-ron không giống hoàn toàn với Perceptron, mà cho kết quả sau khi qua một hàm kích hoạt không tuyến tính, nhưng khả vi (như với Adaline), thì ta không những có thể sử dụng đạo hàm để tối ưu độ lỗi, mà còn có thể dùng quy tắc chuỗi để tính đạo hàm cho tất cả nơ-ron ở lớp trước đó. Đây là cách ta điều chỉnh trọng số của những nơ-ron trước lớp đầu ra cuối cùng. Một cách hiểu đơn giản: ta có thể dùng giải tích để “phạt” bất kỳ lỗi nào trong tập huấn luyện ở lớp đầu ra cho mỗi nơ-ron ở lớp ẩn trước đó, và sau đó chia nhỏ hình phạt này đến một lớp ẩn khác, cứ thế - ta lan truyền ngược độ lỗi, để điều chỉnh tất cả trọng số của mô hình. Vì vậy, ta có thể biết được độ lỗi thay đổi ra sao nếu ta thay đổi bất kỳ trọng số nào trong mạng, bao gồm cả những lớp ẩn, và dùng kỹ thuật tối ưu (trong một thời gian dài mọi người thường dùng Stochastic Gradient Descent) để tìm trọng số tối ưu, và giảm độ lỗi.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/948ce27d6f1346209304d885944491fe/7c09c/39ac3be316cb331f8edfd9e138410c147d08ff46.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.94117647058824%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAQADBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe6GgTH/xAAWEAEBAQAAAAAAAAAAAAAAAAAQATL/2gAIAQEAAQUCZo//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAXEAADAQAAAAAAAAAAAAAAAAAAARAx/9oACAEBAAY/Apo7/8QAGxAAAgEFAAAAAAAAAAAAAAAAAAEQETFRcaH/2gAIAQEAAT8hVsC2UY4hR//aAAwDAQACAAMAAAAQsM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAABBAMBAAAAAAAAAAAAAAABABEhMRBhobH/2gAIAQEAAT8QcDAh4icCZi5QscX0uEqDH//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"39ac3be316cb331f8edfd9e138410c147d08ff46.jpeg\"\n        title=\"39ac3be316cb331f8edfd9e138410c147d08ff46.jpeg\"\n        src=\"/static/948ce27d6f1346209304d885944491fe/7bf67/39ac3be316cb331f8edfd9e138410c147d08ff46.jpg\"\n        srcset=\"/static/948ce27d6f1346209304d885944491fe/651be/39ac3be316cb331f8edfd9e138410c147d08ff46.jpg 170w,\n/static/948ce27d6f1346209304d885944491fe/d30a3/39ac3be316cb331f8edfd9e138410c147d08ff46.jpg 340w,\n/static/948ce27d6f1346209304d885944491fe/7bf67/39ac3be316cb331f8edfd9e138410c147d08ff46.jpg 680w,\n/static/948ce27d6f1346209304d885944491fe/7c09c/39ac3be316cb331f8edfd9e138410c147d08ff46.jpg 975w\"\n        sizes=\"(max-width: 680px) 100vw, 680px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Ý tưởng cơ bản của lan truyền ngược (</em><em><a href=\"http://devblogs.nvidia.com/parallelforall/inference-next-step-gpu-accelerated-deep-learning/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Nguồn</a></em> <em>)</em></p>\n<p>Kỹ thuật lan truyền ngược (backpropagation) được nhiều nhà nghiên cứu đưa ra vào đầu những năm 60 và được cài đặt để chạy trên máy tính như ngày nay vào đầu năm 1970 bởi Seppo Linnainmaa, nhưng Paul Werbos là người đầu tiên ở Mỹ đề xuất rằng nó có thể được sử dụng cho mạng nơ-ron, sau khi ông phân tích chuyên sâu trong luận án tiến sĩ của mình. Thú vị là, như với Perceptron, ông được truyền cảm hứng từ những mô hình tâm trí của con người, trong trường hợp này là học thuyết tâm lý của Freud, như ông đã kể lại:</p>\n<p>Mặc dù đã giải quyết được câu hỏi làm thế nào để huấn luyện mạng nơ-ron nhiều lớp, và hình dung được điều này khi thực hiện luận án tiến sĩ của mình, Werbos đã không công bố về việc ứng dụng lan truyền ngược vào mạng nơ-ron mãi cho đến năm 1982 do tác động của “the AI winter”. Trên thực tế, Werbos nghĩ rằng đây là cách tiếp cận hợp lý để giải quyết các vấn đề còn tồn đọng của Perceptrons, nhưng cộng đồng nhìn chung đã mất niềm tin vào việc giải quyết những vấn đề đó:</p>\n<p>Có vẻ vì sự thiếu quan tâm đến chủ đề học thuật này mà mãi hơn một thập kỷ sau, vào năm 1986, hướng tiếp cận này mới trở nên phổ biến, nhờ vào bài báo “Learning representations by back-propagating errors” (Học các biểu diễn bằng cách lan truyền ngược lỗi) của David Rumelhart, Geoffrey Hinton, và Ronald Williams. Dù có rất nhiều khám phá về phương pháp (bài báo thậm chí còn đề cập rõ David Parker và Yann LeCun là hai người đã khám phá ra nó trước đó), ấn phẩm năm 1986 này nổi bật vì ý tưởng được trình bày ngắn gọn và rõ ràng. Trên thực tế, nếu là một người có hiểu biết về học máy, ta dễ nhận thấy mô tả trong bài báo về cơ bản giống với các khái niệm được trình bày trong sách cũng như các lớp AI ngày nay. Một hồi tưởng trong IEEE lặp lại quan điểm này:</p>\n<p>Nhưng trên cả việc trình bày thuật toán học mới này, họ đã xuất bản một cuốn sách chuyên sâu hơn về “Learning internal representations by error propagation” trong cùng năm đó, giải quyết cụ thể các vấn đề được Minsky thảo luận ở cuốn Perceptrons. Dù ý tưởng đã xuất hiện trong quá khứ, nhưng chính công bố chính thức vào năm 1986 này đã giúp người ta hiểu rộng rãi hơn về cách mạng nơ-ron đa lớp có thể được huấn luyện để giải quyết các vấn đề phức tạp. Và như vậy, mạng nơ-ron đã trở lại! Ở phần tiếp theo, chúng ta sẽ tìm hiểu xem làm thế nào mà chỉ một vài năm sau, lan truyền ngược và một vài thủ thuật khác được thảo luận trong cuốn sách trên đã được áp dụng cho một bài toán quan trọng: giúp máy tính đọc được chữ viết tay.</p>\n<hr>\n<p><em>Dịch từ bài viết</em> <strong><em><a href=\"https://www.skynettoday.com/overviews/neural-net-history\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">A Brief History of Neural Nets and Deep Learning</a></em></strong> <em>, Andrey Kurenkov.</em></p>\n<hr>","frontmatter":{"title":"Lược sử mạng nơ-ron và học sâu (Phần 1)","description":"Câu chuyện về sự phát triển của mạng nơ-ron từ những ngày đầu của trí tuệ nhân tạo đến nay.","image":{"childImageSharp":{"resize":{"src":"/static/d116d1b31c6b0c9c24c0ffefa6c31883/c7a62/8c4ffd0816bfcffd4a14690bc986b35dcaa1ca89.jpg","height":630,"width":1200}}}}}},"pageContext":{"slug":"/luoc-su-mang-no-ron-va-hoc-sau-phan-1/","previousPost":{"title":"Lược sử mạng nơ-ron và học sâu (Phần 2)","slug":"/luoc-su-mang-no-ron-va-hoc-sau-phan-2/"},"nextPost":null}},"staticQueryHashes":["3868140423","4085307986"]}